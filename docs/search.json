[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A curious human being",
    "section": "",
    "text": "Thanks for visiting my space, this where I learn, think, and write.\nMy background is mostly in distributed machine learning systems.\n\nI graduated from University of Michigan (summa cum laude) and Shanghai Jiao Tong University with degrees in both Mechanical Engineering and Electrical Computer Engineering.\nI obtained my Master’s degree from UCLA in Computer Science (specialization: data mining and machine learning).\nI spent 2 years in Meta’s marketplace team working on recommendation systems.\nI spent 2 years in Voleon (largest quant HF in California) working on data and systems.\nI spent 3 years traveling and exploring, learning about the mathematical foundation behind deep learning, implemented various models and architectures, played a lot with GPU kernels.\n\nI currently work at Manifest AI, carrying out research on efficient language modeling.\nI also believe in Longtermism where positively influencing the long-term future is the priority of our time."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "A curious human being",
    "section": "",
    "text": "Thanks for visiting my space, this where I learn, think, and write.\nMy background is mostly in distributed machine learning systems.\n\nI graduated from University of Michigan (summa cum laude) and Shanghai Jiao Tong University with degrees in both Mechanical Engineering and Electrical Computer Engineering.\nI obtained my Master’s degree from UCLA in Computer Science (specialization: data mining and machine learning).\nI spent 2 years in Meta’s marketplace team working on recommendation systems.\nI spent 2 years in Voleon (largest quant HF in California) working on data and systems.\nI spent 3 years traveling and exploring, learning about the mathematical foundation behind deep learning, implemented various models and architectures, played a lot with GPU kernels.\n\nI currently work at Manifest AI, carrying out research on efficient language modeling.\nI also believe in Longtermism where positively influencing the long-term future is the priority of our time."
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "A curious human being",
    "section": "Publications",
    "text": "Publications\n\nKumar, S., Buckman, J., Gelada, C., & Zhang, X. (2025). Conformal Transformations for Symmetric Power Transformers. In First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models. Paper\nGelada, C., Buckman, J., Zhang, S., & Bach, T. (2025). Scaling Context Requires Rethinking Attention. arXiv preprint arXiv:2507.04239. Paper"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "A curious human being",
    "section": "Contact",
    "text": "Contact\nI’m currently based in Vancouver, Canada. I can be reached at sz@seanzhang.me."
  },
  {
    "objectID": "posts/ssm_lack_sequence_mixing/index.html",
    "href": "posts/ssm_lack_sequence_mixing/index.html",
    "title": "SSM lacks sequence mixing",
    "section": "",
    "text": "These are just my thoughts and intuition after tinkering with SSMs for the past month. Take with a grain of salt"
  },
  {
    "objectID": "posts/ssm_lack_sequence_mixing/index.html#opinion",
    "href": "posts/ssm_lack_sequence_mixing/index.html#opinion",
    "title": "SSM lacks sequence mixing",
    "section": "Opinion",
    "text": "Opinion\nEvery architecture contains some implicit trade-offs. My impression is SSMs are a good sequential architecture for modalities where interactions within a sequence matters less than a good compression of past states. However, it might not be the best architecture if the following 2 conditions are met:\n\nThe marginal gain of additional compression quality outweighs the efficiency loss.\nThe way a task depends on past history varies a lot (the definition of “a lot” will become clearer later).\n\nThe 1st condition is fairly self-explanatory and is generally true for complicated deep learning tasks (chatbot, self-driving), at least for the time being, and especially true for areas that are yet to be solved.\n\n\n\nAn illustration of quality-efficiency trade-off\n\n\nThe 2nd condition is more subtle, because what does “the way a task depends on past history” actually mean? Before attempting to answer this question, here’s why I think it matters for SSM models.\nBut even before that, let’s do a quick recap of state space models are."
  },
  {
    "objectID": "posts/ssm_lack_sequence_mixing/index.html#ssm-models",
    "href": "posts/ssm_lack_sequence_mixing/index.html#ssm-models",
    "title": "SSM lacks sequence mixing",
    "section": "SSM Models",
    "text": "SSM Models\nWhen I refer to SSM models, I’m not referring to the classical state space models used in a control context or a quantitative finance context, but rather SSM in the context of sequential neural networks.\n\n\n\nLiterature roadmap of recent State Space neural network Model architectures\n\n\nThe above is literature roadmap of recent SSM architectures. The are some terrific detailed explanation on what they are:\n\nThe Annotated S4 dives deep into S4 and explained the inner workings of S4.\nA Visual Guide to Mamba and State Space Models explained, visually, what mamba does.\nFrom Deep to Long Learning? where the authors themselves gave a good account of the development of H3 and Hyena.\n\nIn short, SSM is the following process that models state changes of a system\n\\[\n\\begin{aligned}\n\\color{red}h_{t+1} &= {\\color{blue}A}\\cdot {\\color{red}{h_t}} + {\\color{green}{B\\cdot x_t}}\\\\\ny_t &= C\\cdot {\\color{red}{h_t}} + D\\cdot x_t \\\\\n\\text{where } &t := \\text{Step/Time}\\\\\n&h := \\text{Hidden state of the system} \\\\\n&x := \\text{Input to the system}\\\\\n&y := \\text{Output of the system} \\\\\n&A := \\text{A matrix determining how the state changes over }t \\\\\n&B := \\text{How input affects state} \\\\\n&C := \\text{How state affects output} \\\\\n&D := \\text{How input directly affects output}\n\\end{aligned}\n\\]\n(The above is only for discrete problem, similar formulation for continuous time; Also I’m neglecting the discretization step here for illustration purpose)\nThere’re many properties to this model, and its usefulness is immense in the field of control, signal processing, time series application, etc.\n\nHistory preserving\nJust by laying out the equations above does not guarantee a good compression of past history. The magic of SSMs comes from the theory of approximation theory, and in particular, orthogonal polynomials (Chihara 1978). Consider a scale value \\(x\\) that varies with sequence \\(x\\), i.e.,\n\\[\nf = f(x)\n\\]\nSuppose we want to approximate the history with a limited number of numbers \\(c_1, c_2, ... c_N\\), one way to achieve it is to let each \\(c_n\\) correspond to a “basis function” \\(\\phi_n\\) and let\n\\[\n\\hat f(x) = \\sum_{n=0}^N c_n\\phi_n(x)\n\\]\nThe approximation error is therefore defined as\n\\[\n\\begin{aligned}\n\\text{Error} &= ||f(x) - \\hat f(x)||^2_w = \\int_0^\\infty(f(x) - \\hat f(x))^2w(x)dx\\\\\n\\text{where } w & \\text{ defines a weighting function}\n\\end{aligned}\n\\]\nThe weighting function is added to further generalize the discussion. Expanding the error, one obtains\n\\[\n\\begin{aligned}\n\\mathcal{L} &= \\int_0^\\infty(f(x) - \\sum_{n=0}^Nc_n\\phi_n(x))^2w(x)dx \\\\\n&=\\int_0^\\infty\\left[f^2(x) + \\sum_{i=0}^Nc_i^2\\phi_i^2(x) - 2\\sum_{i\\neq j}c_ic_j\\phi_i(x)\\phi_j(x) - 2\\sum_{i=0}^Nc_i\\phi_i(x)f(x)\\right]w(x)dx\n\\end{aligned}\n\\]\nIn practice, we don’t have to limit ourselves to integrating from \\(0\\) to \\(\\infty\\). This motivates defining the inner product in the function space\n\\[\n\\langle f, g\\rangle_w := \\int_a^b f(x)g(x)w(x)dx\n\\]\nThe approximation error can be further rewritten as\n\\[\n\\mathcal{L} = \\langle f, f\\rangle + \\sum_{i}^N c_i^2\\langle\\phi_i, \\phi_i\\rangle + \\sum_{i \\neq j} c_ic_j\\langle \\phi_i, \\phi_j \\rangle - 2\\sum_{i}^Nc_i\\langle \\phi_i, f\\rangle\n\\]\nNow, wouldn’t it be nice if we can eliminate some of the terms here? In fact, we can do exactly that with orthogonal polynomials\\(^2\\).\n\n\nOrthogonal Polynomials\nOrthogonal polynomials have the property that they are orthogonal to each other under the inner product definition, i.e.,\n\\[\n\\langle \\phi_i, \\phi_j \\rangle = 0, \\forall i \\neq j\n\\]\nOne example would be the Legendre polynomials, which is defined over \\(t \\in [-1, 1]\\) with \\(w(t) = 1\\), and is of the following forms\n\\[\n\\begin{aligned}\n\\phi_0(x) &= 1\\\\\n\\phi_1(x) &= x\\\\\n\\phi_2(x) &= \\frac{1}{2}(3x^2 - 1)\\\\\n\\phi_3(x) &= \\frac12(5x^3 - 3x) \\\\\n...\\\\\n\\phi_n(x) &= \\frac{1}{2^nn!}\\frac{d^n}{dx^n}(x^2-1)^n\n\\end{aligned}\n\\]\n(note: there’s one unique set of OPs for any weight function for any given interval)\nWith this property, the approximation error simplifies to (the 3rd term disappears)\n\\[\n\\mathcal{L} = \\langle f, f\\rangle + \\sum_{i}^N c_i^2\\langle\\phi_i, \\phi_i\\rangle - 2\\sum_{i}^Nc_i\\langle \\phi_i, f\\rangle\n\\]\nTo simplify it further, let’s define the constant in a meaningful way by taking the gradient of \\(\\mathcal{L}\\) with respect to \\(c_i\\) and set it to 0\n\\[\n\\begin{aligned}\n\\nabla_{c_i}\\mathcal{L} &= 2c_i\\langle\\phi_i, \\phi_i\\rangle - 2\\langle\\phi_i, f\\rangle = 0 \\\\\n\\rightarrow c_i &=\\frac{\\langle\\phi_i, \\phi_i\\rangle}{\\langle\\phi_i, f\\rangle}\n\\end{aligned}\n\\]\nThe above gives us a way to combine a set of orthogonal polynomials to achieve minimum approximation error with respect to any function \\(f\\).\n\n\nMake the weight time-dependent\nNow, what if the weighting function is also time dependent (changes over time)? Instead of \\(w(x)\\), we have \\(w^{(t)}(x)\\). This leads to a more complicated system where everything should be defined with respect to another time. To save you some time, this leads to the following equation for the “minimization coefficient”\n\\[\nc_n(t) = \\zeta(t)^{-\\frac{1}{2}}\\lambda_n \\int fp_n^{(t)}\\frac{w^{(t)}}{\\mathcal{\\chi}^{(t)}}\n\\]\nwhere \\(p_n^{(t)}\\) is the basis OP used in the system, \\(\\chi^{(t)}(x)\\) is a scaling function to increase the generality of the argument, and \\(\\zeta(t)\\) is a normalization term caused by \\(\\chi^{(t)}\\), \\(w^{(t)}\\) is the time-varying weighting function.\nThe difference between \\(t\\) and \\(x\\) is plotted below.\n\n\n\nThe difference between \\(t\\) and \\(x\\)\n\n\nNow, why on earth would we want to do this? We want to do this because we want to take the derivative of \\(c_n(t)\\) with respect to \\(t\\), and hopefully derive a SSM out of it. We can do exactly that\n\\[\n\\begin{aligned}\\frac{d}{d t} c_n(t)= & \\zeta(t)^{-\\frac{1}{2}} \\lambda_n \\int f(x)\\left(\\frac{\\partial}{\\partial t} p_n(t, x)\\right) \\frac{\\omega}{\\chi}(t, x) \\mathrm{d} x \\\\& +\\int f(x)\\left(\\zeta^{-\\frac{1}{2}} \\lambda_n p_n(t, x)\\right)\\left(\\frac{\\partial}{\\partial t} \\frac{\\omega}{\\chi}(t, x)\\right) \\mathrm{d} x .\\end{aligned}\n\\]\nThe beauty here is that \\(\\frac{\\partial}{\\partial t} p_n(t, x)\\) and \\(\\frac{\\partial}{\\partial t} \\frac{\\omega}{\\chi}(t, x)\\) can both be expressed in close-form and related back to themselves, which means we get an ODE out of this!\nOnce an ODE is obtained, we can structure it to form an SSM. Note that the precise form of SSM depends on what weighting function/OP we use. Different choice of weighting function represents how we weight the history.\nHere’s an example of the explicit form of SSM, for the Laguerre polynomials\n\\[\n\\begin{aligned}\\frac{d}{d t} c(t) & =-A c(t)+B f(t) \\\\A & =\\left[\\begin{array}{cccc}\\frac{1+\\beta}{2} & 0 & \\ldots & 0 \\\\1 & \\frac{1+\\beta}{2} & \\ldots & 0 \\\\\\vdots & & \\ddots & \\\\1 & 1 & \\ldots & \\frac{1+\\beta}{2}\\end{array}\\right] \\\\B & =\\zeta^{-\\frac{1}{2}} \\cdot\\left[\\begin{array}{c}\\left(\\begin{array}{c}\\alpha \\\\0\\end{array}\\right) \\\\\\vdots \\\\\\left(\\begin{array}{c}N-1+\\alpha \\\\N-1\\end{array}\\right)\\end{array}\\right]\\end{aligned}\n\\]\n\n\nProblem?\nAfter defining the SSM in the continuous domain, one needs to discretize it and turn it into actual architecture and code. However, here I want to focus on 2 properties of SSM:\n\n\n\n\n\n\nNote\n\n\n\n💡 1. The way that \\(\\color{red}{h_t}\\) evolves by itself is time invariant 2. The way \\(\\color{brown}x_t\\) affects \\(\\color{red}h_t\\) is additive\n\n\n(here I’m abusing notations by using \\(h_t\\) and \\(c(t)\\) interchangeably, and using \\(x(t)\\) for \\(f(t)\\))\nTo elaborate a bit, it means\n\nWithout external input, this is a deterministic linear system (by design), and it’s much computationally easier to compute a linear system (i.e., convolution can be applied)\nThe expressiveness of the system is bound by linearity\n\nTo see why the 2nd point is true, we can expand the system equation and obtain\n\\[\n\\begin{aligned}\nh_{t+1} &= A h_{t} + Bx_{t} \\\\\n&= A^2h_{t-1} + ABx_{t-1} + Bx_{t} \\\\\n&= A^th_1 + A^{t-1}Bx_1 + A^{t-2}Bx_2 + ... + Bx_t\n\\\\\ny_{t} &= CA^th_1 + CA^{t-1}Bx_1 + CA^{t-2}Bx_2 + ... + CBx_t + Dx_t\n\\end{aligned}\n\\]\nwhere it’s clear \\(x_{1:t}\\) can only affect the system state \\(h\\) in linear fashion, if \\(A\\) and \\(B\\) are fixed. Also, if \\(C\\) and \\(D\\) are fixed, \\(y_t\\) is also affected by \\(x_{1:t}\\) in a linear fashion.\n\n\nLack of non-linearity\nIt’s clear that such a system, while might be desirable for system control, is not ideal to express a complicated system that is non-linear. And it’s hard to argue that tasks like language modeling will be a linear system. In the most recent architectures (such as Mamba(Gu and Dao 2024)), this lack of expressiveness is addressed by\n\nreplacing \\(B\\) with \\(B(x_t)\\)\nreplacing \\(C\\) with \\(C(x_t)\\)\nreplacing \\(D\\) with \\(D(x_t)\\)\n\nTherefore, the governing equation is replaced by\n\\[\n\\begin{aligned}\n\\color{red}h_{t+1} &= {\\color{blue}A}\\cdot {\\color{red}{h_t}} + {\\color{green}{B(x_t)\\cdot x_t}} \\\\\ny_t &= C(x_t)\\cdot {\\color{red}{h_t}} + D(x_t)\\cdot x_t\\\\\n\\end{aligned}\n\\]\nwhere we can fold the input \\(x_t\\) further into \\(B(x_t)\\) and \\(D(x_t)\\) without loss of generality, and obtain\n\\[\n\\begin{aligned}\n\\color{red}h_{t+1} &= {\\color{blue}A}\\cdot {\\color{red}{h_t}} + {\\color{green}{B(x_t)}} \\\\\ny_t &= C(x_t)\\cdot {\\color{red}{h_t}} + D(x_t)\\\\\n\\end{aligned}\n\\]\nSimilarly, we can expand the system again and obtain\n\\[\n\\begin{aligned}\nh_{t+1} &= A^th_1 + A^{t-1}B(x_1) + A^{t-2}B(x_2) + ... + B(x_t)\n\\\\\ny_{t} &= C(x_t)A^{t-1}h_1 + C(x_t)A^{t-1}B(x_2) + C(x_t)A^{t-2}B(x_3) + ... + C(x_t)B(x_t) + D(x_t)\n\\end{aligned}\n\\]\nAlthough this is technically a non-linear system as long as \\(B(x)\\) or \\(C(x)\\) or \\(D(x)\\) is non-linear (for example, \\(B(x) = \\sin(x)\\)), this modeling does not involve any sequence-crossing terms like \\(B(x_1)\\cdot B(x_2)\\).\nThis, in my opinion, hugely impacts the expressiveness of the system because the lack of sequence-cross in sequential modeling is similar to the lack of feature-crossing in tabular modeling, which will result in low sample efficiency.\n\n\nMamba to the rescue?\nIn the Mamba\\(^1\\) paper, the authors (I’d argue partially) addressed this issue by:\n\ninserting a convolution layer before \\(x\\), but I don’t see how that will fundamentally change the picture, because \\(x_i\\) and \\(x_j\\) will not interact with each other if their sequential distance is larger than the convolution kernel size.\nhaving more layers so \\(x_i\\) and \\(x_j\\) can interact with each other at a higher layer, but this does not change the fact that on a given layer, no cross-sequence interaction can happen.\ncreating a gating layer so that an explicit sequence-crossing layer is added to the output \\(y_t\\), i.e.,\n\\[\n\\begin{aligned}\n\\text{Instead of }\\quad y_t &= C(x_t)h_t + D(x_t) \\\\\n\\text{Let}\\quad y_t&=\\sigma(x_{1:t})*(C(x_t)h_t + D(x_t))\n\\end{aligned}\n\\]\nwhich technically makes sequence-crossing possible. This is similar to GRU and LSTM. The additional expressiveness of such a gating mechanism, one can argue, is much less that attention, because the gate itself does not contain sequence-crossing terms."
  },
  {
    "objectID": "posts/ssm_lack_sequence_mixing/index.html#intuition",
    "href": "posts/ssm_lack_sequence_mixing/index.html#intuition",
    "title": "SSM lacks sequence mixing",
    "section": "Intuition",
    "text": "Intuition\nBy turning the linear SSM into an non-linear SSM, and adding these 3 additional modifications, SSM seem to perform well across many modalities (arguments can be made on the details of some of these experiment results, such as the extremely low vocab size used in the induction head(Olsson et al. 2022) task).\nHowever, a pattern has emerged through the evolution of SSM models where we started off with a beautiful mathematical model (a unified compression scheme) that has theoretical guarantees on approximation error and implementation efficiency. However, due to our limited understanding of the expressiveness required for complicated domains like language, an iterative approach must be taken to move the trad-off point (illustrated below) towards one that uses the minimum complexity to achieve the required expressiveness and completeness.\nThe question then becomes “is transformer already at the best trade-off point”? For most domains?\n\n\n\nA visual illustration of the trade-off point\n\n\nI have no idea. (I have some ideas now!)"
  },
  {
    "objectID": "posts/em_in_a_nutshell/index.html",
    "href": "posts/em_in_a_nutshell/index.html",
    "title": "EM in a nutshell",
    "section": "",
    "text": "One of the most interesting ideas in machine learning I’ve found is the EM algorithm. The idea behind EM is summarized as follows:"
  },
  {
    "objectID": "posts/em_in_a_nutshell/index.html#limitations",
    "href": "posts/em_in_a_nutshell/index.html#limitations",
    "title": "EM in a nutshell",
    "section": "Limitations",
    "text": "Limitations\nWhile the idea of EM is powerful, it is impractical in models where the evaluation of the posterior \\(p(Z|\\theta, X)\\) is impossible (think multi-layer deep neural network with nonlinearity in between).\nTo make inference about \\(Z\\) in those cases, we need to resort to another powerful tool (Variational Inference)."
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html",
    "href": "posts/hitchhiker_cuda/index.html",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "",
    "text": "In this post I introduce the CUDA programming ecosystem by writing a kernel achieves 95% of cuBLAS’s SGEMM performance. I hope this helps those are just getting started in CUDA programming."
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#gpu-hardware",
    "href": "posts/hitchhiker_cuda/index.html#gpu-hardware",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "GPU Hardware",
    "text": "GPU Hardware\n\n\n\nA simplified overview of GPU architecture, numbers are for modern (as of 2024) GPU SM: Streaming Multiprocessor, the first-level “parallel component” in GPU, there’re 128 SM in a RTX4090 HBM: High Bandwidth Memory, the “global memory”/”device memory” of GPU, size is usually around 20GB~80GB. Communicating with this memory is considered extremely slow, on the order of 400-800 cycles per access. L2 cache: The cache for HBM, read and writes to HBM will be cached here. Communication with this memory is much faster (but Nvidia doesn’t disclose it publicly), a ballpark number for is 2TB/s. SMSP: Streaming Multiprocessor Sub-Partition, introduced in the Ampere architecture, the second-level “parallel component” in GPU, there’re 4 SMSP in an SM. L1 cache: Each SM has its own L1 cache (smaller), also used to cache read and writes to HBM. Read/write to this memory usually takes 20-40 cycles. Shared Memory: Each SM has a chunk of memory for all threads/processes running on all of its SMSP. Read/write to this memory usually takes 20-40 cycles as well. Register File: A chunk of registers local to each SMSP (and local to the threads running on it). Read/write to registers takes 1 cycle (as fast as you can get). Warp Scheduler: The scheduler that schedules which warp gets run on the SMSP at any given time, more on this later. INT32: The arithmetic “core” that performs 32-bit integer arithmetics. This core works on scalar values. FP32: The arithmetic “core” that performs 32-bit floating point arithmetics. This core works on scalar values. Tensor Cores: The arithmetic “core” that can both perform floating point arithmetics and integer arithmetics. This core works on a bunch of values at a time (hence the name tensor)\n\n\nThe above diagram hopefully depicts what a modern (Nvidia) GPU consists of. There’re a few things to note here about the hardware:\n\nBoth L1 cache and L2 cache are “invisible” to programmers in the sense that you can’t directly manipulate them (but you can set the persistence size of L2 cache, or give hints to the machine w.r.t. the cache policy)\nL1 cache and Shared Memory actually the same physical memory, it’s up to the programmer to configure the size of each.\nThe bandwidth between each component varies greatly from GPU to GPU, but generally\n\\[\n\\text{HBM} &lt; \\text{L2} &lt;\\text{L1} = \\text{Shared Memory} &lt; \\text{Register File}\n\\]\nDifferent GPU have different configurations of cores\nTensor core does not support fp32 operations\n\nWhat we can immediately conclude from this is that we want to avoid communication with HBM by caching data aggressively.\n\nMemory Hierarchy\nLet’s take a closer look at the memories in a GPU. The following diagram demonstrates Nvidia GPU’s memory hierarchy, notice how each SMSP has its own registers, SMSPs in the same SM shares the the same shared memory and L1 cache. This is why thread blocks are assigned to SM, not SMSP.\n\n\n\nMemory hierarchy of A100"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#gpu-execution-model",
    "href": "posts/hitchhiker_cuda/index.html#gpu-execution-model",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "GPU Execution Model",
    "text": "GPU Execution Model\nAnother important aspect of the GPU hardware is to understand how it executes.\n\nSIMT\nThe execution model follows a SIMT model (as opposed to SIMD or other forms of parallel computing model), this roughly means every line of code you write gets executed on multiple threads, in parallel, as opposed to explicitly telling the program how to parallelize the workload.\nThe following examples demonstrate 2 cases where you explicitly tell the program how to do the parallelization.\nfrom multiprocessing import Pool\n\ndef f(x):\n    return x*x\n\nwith Pool(processes=4) as pool:\n    result = pool.map(f, range(10))\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;omp.h&gt;\n\nint f(int x) {\n    return x * x;\n}\n\nint main() {\n    const int num_elements = 10;\n    std::vector&lt;int&gt; result(num_elements);\n\n    // Parallelize the loop with OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i &lt; num_elements; ++i) {\n        result[i] = f(i);\n    }\n}\nThe following example demonstrates CUDA’s SMIT model, just read the highlighted part.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\n// This is the device code, runs in GPU\n**// IMPORTANT: Every thread executes the same program, you don't have for loops**\n__global__ void square(int *d_result, int num_elements) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i &lt; num_elements) {\n        d_result[i] = i * i;\n    }\n}\n\n// This is the host code, runs in CPU\nint main() {\n    const int num_elements = 10;\n    std::vector&lt;int&gt; result(num_elements);\n\n    int *d_result;\n    cudaMalloc((void**)&d_result, num_elements * sizeof(int));\n\n    int blockSize = 256;\n    int numBlocks = (num_elements + blockSize - 1) / blockSize;\n    **// This launchs the CUDA kernel on the GPU\n    // NOTE: this is non-blocking**\n    square&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(d_result, num_elements);\n\n    cudaMemcpy(result.data(), d_result, num_elements * sizeof(int), cudaMemcpyDeviceToHost);\n\n    cudaFree(d_result);\n\n    // Print the result\n    for (int i = 0; i &lt; num_elements; ++i) {\n        std::cout &lt;&lt; result[i] &lt;&lt; \" \";\n    }\n    std::cout &lt;&lt; std::endl;\n\n    return 0;\n}\nA reasonable question to ask is, if every thread is executing the same code, how is it going to solve any useful task? The subtlety here is that although each thread is executing the same code, the data pointed by addresses in each thread is different.\n\n\nExecution Hierarchy\nGiven the hardware hierarchy above, CUDA provides an execution hierarchy as follows\n\n\n\nSimplified CUDA execution hierarchy\n\n\nIn CUDA, the execution of every problem/kernel program is represented as a set of thread blocks (called the grid). The way this division of the problem is done is managed by the programmer, for example, to do matrix multiplication of two 4096 * 4096 matrix, one can choose to assign the calculation of every 128 * 128 output to one block, effectively giving 1024 blocks (4096^2 / 128^2).\nEach block is assigned to one and only one SM (streaming multiprocessor), but multiple blocks can be assigned to one SM. The exact block-to-block distribution policy is not made public by Nvidia, but according to [3], the default distribution policy is round-robin (provided that there’s only one stream). Also, because each block will take up resources (register files, shared memory), the number of blocks (also called occupancy) that can be assigned to a single SM is limited by each block’s resource usage. As we’ll see later, we want higher occupancy in order to hide memory latency.\nAnother important concept in CUDA is called a warp, which basically means a group of 32 threads. A block can contain a configurable number of warps. The idea of warp is important because it’s the smallest scheduling primitive in CUDA.\n\n\n\nWarp is the smallest scheduling primitive, an SMSP can only execute instructions for one warp at a time. The “.add” here simply symbolizes an addition operation carried out by each warp.\n\n\n\n\n\nEvery thread in a warp always executes the same thing (mostly, thread divergence can happen, sometimes intentionally by the programmer, in which case the execution is serialized by masking one group, execute; then masking another group, execute. This usually means degraded performance)\n\n\nEvery thread in a warp always executes the same thing (mostly, thread divergence can happen, sometimes intentionally by the programmer, in which case the execution is serialized by masking one group, execute; then masking another group, execute. This usually means degraded performance)\n\n\nLatency Hiding\nThe left image above shows how different warps execute sequentially, but they are all doing arithmetic operations (addition), which is only part of the picture. Let’s see what happens if we try to depict a more realistic picture, i.e., we should load data from/to memory before doing addition.\n\n\n\nA more realistic depiction of different warp carrying out memory loading before doing addition.\n\n\nAn important realization is that memory operation takes much longer time (~400 cycles) than arithmetic operations (~20 cycles [4]), so if there’s not enough warps on an SM that the scheduler can issue instructions to (because all of them are waiting for the memory load to complete, as all of their next operation depends on this), there will be cycles where no instructions are issued by the warp scheduler. This is very bad for performance because we are essentially wasting hardwares.\nNow consider what happens if we have more warps\n\n\n\nMore warps hide memory latency, no wasted cycles!\n\n\nAs more warps are assigned to an SM, the parallelism allows us to hide the latency of memory load, because we are always doing some work. Note that the parallelism can come from more warps per block, or more blocks. The difference that makes is that warps from different blocks can’t talk to each other via shared memory.\n\n\nOccupancy\nOccupancy measures the ratio between the number of active warps on an SM and the theoretical limit, i.e.,\n\\[\n\\text{Occupancy} = \\frac{\\text{Active concurrent warps}}{\\text{Theoretical limit of number of active warps in an SM}}\n\\]\nUsually, the higher the occupancy, the more opportunity that the SM scheduler has in terms of hiding latency. However, importantly, this does not universally lead to more efficient program because more warps means smaller register usage per warp, and register access is much faster than shared memory access. More on this can be found on (Nvidia 2010).\nTo get the numerator, we have to profile the kernel and obtained from measuring the warp scheduler’s issuance statistics. Usually this done via Nvidia’s nsight compute (in the old days it’s nvprof, but that’s long gone).\nThe denominator is fixed once the programmer determines the kernel’s launch configuration (block size, grid size, etc.), at compile time. Note that the theoretical limit has another upper bound regardless of your launch configuration, i.e., the hardware-defined maximum number of resident warps per SM (look for the Maximum number of resident warps per SM term on (Nvidia 2024)). The latest number for Hopper architecture is 64.\nAt compile time, the denominator is determined by mainly 3 resources as shown below\n\n\n\n\nBlock Resource Required\nLimiting Factor\n\n\n\n\n1\nShared Memory per Block\nShared Memory per SM\n\n\n2\n# Threads per block\n# Threads limit per SM\n\n\n3\n# Register usage per thread\nRegister File per SM\n\n\n\nConcretely, the number of blocks that can be assigned to an SM is determined by taking the minimum of \\(\\frac{\\text{Total Resource}}{\\text{Resources Required per Block}}\\) for all these 3 resources, meaning\n$$\n\\[\\begin{gathered}\\text { Theoretical Limit }=\\\\\\min\\left(\\lfloor\\frac{\\text{SHM/ SM}}{\\text{SHM / block}}\\cdot\\text{Warp/block}\\rfloor, \\lfloor\\frac{\\text{MaxThreads/SM}}{\\text{WarpSize}}\\rfloor, \\lfloor\\frac{\\text{Reg Size/SM}}{\\text{Reg/thread} \\cdot \\text{WarpSize}}\\rfloor\\right)\\end{gathered}\\]\n$$\nnotations are as follows\n\nMaxThreads : the maximum number of threads an SM can take\nSHM: shared memory in bytes\nWarp/Block: number of warps per block, or blockSize/warpSize\nWarpSize: number of threads per warp, always 32 in all Nvidia GPU so far\nReg/thread: register usage per thread\n\nDue to the importance of occupancy, there both official and unofficial tools to calculate the theoretical limit.\n\n\n\ntheoretical #warps depending on different launch configuration/resource usage, from CUDA Occupancy Calculator (xmartlabs.github.io)\n\n\nNote how the #warps look like step change functions as the resource usage changes. This is because the \\(\\lfloor \\quad \\rfloor\\) operation we did, as #warps needs to be an integer. The implication is that for a given level of occupancy, there’s a range of resource allocation we can give to each thread without affecting occupancy."
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#naive",
    "href": "posts/hitchhiker_cuda/index.html#naive",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Naive",
    "text": "Naive\nThe naive algorithm to solve the matrix multiplication is demonstrated by both the mathematical definition as well as the diagram below\n\n\n\nNaive matmul\n\n\n\\[\n\\begin{gathered}C_{ij}  = \\sum_{k=1}^{K}A_{ik}B_{kj}\\end{gathered}\n\\]\nThe above shows the most essential operation in matmul, but in practice, people usually do\n\\[\nC = \\alpha\\cdot(A \\times B) + \\beta\\cdot C\n\\]\nwhich is a multiply-accumulation (MAC) operation with 2 scaling factors.\nThe following code shows the implementation of the naive matmul operation.\n#include &lt;cuda_runtime.h&gt;\n\n#define DIV_CEIL(M, N) int(((M) + (N)-1) / (N))\n\n__global__ void sgemm_naive(int M, int N, int K, float alpha, const float *A,\n                            const float *B, float beta, float *C) {\n  const uint x = blockIdx.x * blockDim.x + threadIdx.x;\n  const uint y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (x &lt; M && y &lt; N) {\n    float tmp = 0.0;\n    for (int i = 0; i &lt; K; ++i) {\n      tmp += A[x * K + i] * B[i * N + y];\n    }\n    C[x * N + y] = alpha * tmp + beta * C[x * N + y];\n  }\n}\n\nint main(int M, int N, int K) {\n...\n\ndim3 blockDim(32, 32);\ndim3 gridDim(DIV_CEIL(M, 32), DIV_CEIL(N, 32));\nsgemm_naive&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(M, N, K)\n\n...\n}\nRunning the above code gives us the following poor performance.\n\n\n\nNaive matmul performance\n\n\nTo understand why the performance is poor, consider the arithmetic intensity of this kernel:\n\\[\n\\begin{gathered}\n\\text{Arithmetic Intensity} = \\frac{\\text{Bytes Computed}}{\\text{Bytes I/IO}}\\\\\n= \\frac{1(C_{ij}) \\times 4 (\\text{bytes per float})}{(M(A_{i:})+ N(B_{:j}))\\times 4(\\text{bytes per float})}\\\\\n=\\frac{1}{2K}\n\\end{gathered}\n\\]\nwhich is very low. This is saying, for every \\(2K\\) elements read, we are only calculating \\(1\\) element from the memory read. This is bad because memory read is expensive, and we ideally want to do more work on the read data than just computing 1 number.\nThis can be verified by profiling this kernel with nsight compute and checking the roofline model:\n\n\n\nAs can be seen, the arithmetic intensity of this kernel is too low, resulting 6% of theoretical fp32 performance achieved\n\n\nNot only is the arithmetic intensity low, attention should also be paid to the 2 purple lines above\ntmp += A[x * K + i] * B[i * N + y];\n...\n    C[x * N + y] = alpha * tmp + beta * C[x * N + y];\nThe first line reads data from the device/global memory, without touching the shared memory at all, and the second line read from and write to device/global memory. Both lines are going to be very slow in GPU context. This will result in a very high throughput, but very bad performance. This is an example showing that maximizing throughput should never be the only optimization goal.\n\n\n\nThis naive kernel achieves full throughput in both memory and compute! but it’s very bad algorithm"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#shared-memory-caching",
    "href": "posts/hitchhiker_cuda/index.html#shared-memory-caching",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Shared Memory Caching",
    "text": "Shared Memory Caching\nSince we are not using shared memory, let’s try to resolve this first by using shared memory to cache the data before doing calculation, which results in the following algorithm.\n\n\n\nNaive implementation + smem cache BM and BN are parameters determining how we partition the C matrix by blocks BK is the parameter determining how much data we load from global memory to shared memory every time. We have to have this BK parameter because shared memory is very limited, a whole BMxK block of data usually doesn’t fit\n\n\nAdding smem access gives us the following performance, which is not that impressive. This is because adding smem access does not change the arithmetic intensity. The reason we still see some benefit in small matrix sizes is likely due to the fact that it increases L2 cache hit rate and the memory access is more coalesced as we are loading larger chunk of memories.\n\n\n\nShared memory cached matmul performance\n\n\n\nSmem access kernel\ntemplate&lt;const int BLOCK_SIZE&gt;\n__global__ void sgemm_smem(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) {\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    const int BM = BLOCK_SIZE;\n    const int BN = BLOCK_SIZE;\n    const int BK = BLOCK_SIZE;\n\n    int tx = threadIdx.x % BN;\n    int ty = threadIdx.x / BN;\n\n    __shared__ float As[BM * BK];\n    __shared__ float Bs[BK * BN];\n\n    A = &A[by * BM * K];\n    B = &B[bx * BN];\n    C = &C[by * BM * N + bx * BN];\n\n    float tmp = 0.;\n    for (int k = 0; k &lt; K; k += BK) {\n        As[ty * BK + tx] = A[ty * K + tx];\n        Bs[ty * BN + tx] = B[ty * N + tx];\n        __syncthreads();\n        A += BK;\n        B += BK * N;\n        for (int i = 0; i &lt; BK; i++) {\n            tmp += As[ty * BK + i] * Bs[i * BN + tx];\n        }\n        __syncthreads();\n    }\n    C[ty * N + tx] = alpha * tmp + beta * C[ty * N + tx];\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#thread-tiling",
    "href": "posts/hitchhiker_cuda/index.html#thread-tiling",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Thread Tiling",
    "text": "Thread Tiling\nNow let’s try to improve the arithmetic intensity. We can do this because for each \\(A_{i:}\\) we read, we can theoretically calculate the entire row of \\(C_{i:}\\), provided that we have the corresponding columns of \\(B\\) read in. So as opposed to calculating a single element in \\(C\\), we can theoretically calculate a whole row in \\(C\\). But that’s too extreme, in practice, we can do something called thread tiling:\n\n\n\nThread tiling + Shared memory access\n\n\nThe immediate benefit can be seen by calculating the arithmetic intensity again:\n\\[\n\\begin{gathered}\n\\text{Arithmetic Intensity} = \\frac{\\text{Bytes Computed}}{\\text{Bytes I/IO}}\\\\\n= \\frac{TM \\times TN \\times 4 (\\text{bytes per float})}{(K\\times TN +  K\\times TM)\\times 4(\\text{bytes per float})}\\\\\n=\\frac{TM\\times TN}{K\\times(TM + TN)}\n\\end{gathered}\n\\]\nAnd therefore the arithmetic intensity improvement is\n\\[\n\\text{Improvement} = \\frac{\\frac{TM\\cdot TN}{K\\cdot(TM + TN)}}{\\frac{1}{2K}} = \\frac{2}{1/TM + 1/TN}\n\\]\nfor squared matrices, this can be further simplified\n\\[\n\\text{Improvement} = \\frac{2}{2/TM} = TM\n\\]\nwhich means the arithmetic intensity improvement is proportional to the size of the thread tiling. Note that we can’t arbitrarily make \\(TM\\) as large as we want for the following reason:\n\nA \\(LM/LN\\) too large means less threads/warps, which causes occupancy to drop, therefore hinders performance\nA \\(LM/LN\\) too large means more register usage per warp, which also causes occupancy to drop\nA \\(LM/LN\\) too large can even cause register to overflow to local storage (on global memory), thereby significantly degrading performance\n\nWithout much tuning, we are able to get the following performance with thread tiling:\n\n\n\nThread tiling performance\n\n\nNow we are talking 🙂\n\nThread tiling code\n#pragma once\n\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cublas_v2.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntemplate&lt;const int BM,\n        const int BN,\n        const int BK,\n        const int TM,\n        const int TN&gt;\n__global__ void mysgemm_v4(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) {\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    int block_row_thread = BN / TN;\n    int block_col_thread = BM / TM;\n    int thread_num = block_row_thread * block_col_thread;\n\n    int tx = (threadIdx.x % block_row_thread) * TN;\n    int ty = (threadIdx.x / block_row_thread) * TM;\n\n    __shared__ float As[BM * BK];\n    __shared__ float Bs[BK * BN];\n\n    A = &A[by * BM * K];\n    B = &B[bx * BN];\n    C = &C[by * BM * N + bx * BN];\n\n    int a_tile_row = threadIdx.x / BK;\n    int a_tile_col = threadIdx.x % BK;\n    int a_tile_stride = thread_num / BK;\n\n    int b_tile_row = threadIdx.x / BN;\n    int b_tile_col = threadIdx.x % BN;\n    int b_tile_stride = thread_num / BN;\n\n    float tmp[TM][TN] = {0.};\n    for (int k = 0; k &lt; K; k += BK) {\n        for (int i = 0; i &lt; BM; i += a_tile_stride) {\n            As[(a_tile_row + i) * BK + a_tile_col] = A[(a_tile_row + i) * K + a_tile_col];\n        }\n        for (int i = 0; i &lt; BK; i += b_tile_stride) {\n            Bs[(b_tile_row + i) * BN + b_tile_col] = B[(b_tile_row + i) * N + b_tile_col];\n        }\n        __syncthreads();\n        A += BK;\n        B += BK * N;\n        for (int i = 0; i &lt; BK; i++) {\n            for (int j = 0; j &lt; TM; j++) {\n                for (int l = 0; l &lt; TN; l++)\n                    tmp[j][l] += As[(ty + j) * BK + i] * Bs[tx + l + i * BN];\n            }\n        }\n        __syncthreads();\n    }\n    for (int j = 0; j &lt; TM; j++) {\n        for (int l = 0; l &lt; TN; l++)\n            C[(ty + j) * N + tx + l] = alpha * tmp[j][l] + beta * C[(ty + j) * N + tx + l];\n    }\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#vectorized-access-coalesced-access",
    "href": "posts/hitchhiker_cuda/index.html#vectorized-access-coalesced-access",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Vectorized Access & Coalesced Access",
    "text": "Vectorized Access & Coalesced Access\nOne of the common optimization method is (Nvidia 2013), which is basically saying instead of getting a single float at a time, we want to get 2, or 4 floats at a time. This improves performance because\n\nIn CUDA, all device/global memory access is done via 32-, 64-, or 128-byte memory transactions\nFor threads within a warp accessing global memories, their access will be grouped together in 32-, 64-, or 128-byte memory transactions, after data is read from memory, data for the warp will be distributed onto individual threads\nIf we are getting 4 consecutive 32-byte memory (8 floats) from a warp, we are issuing 4 instructions (the LD instruction); but if we are getting one 128-byte from a warp, we are only issuing 1 instruction (the LD.128 instruction). There’re 2 benefits:\n\nSmaller number of instructions ⇒ instruction cache hit rate will improve\nDespite that the issuance of load instruction itself takes small number of cycles (majority of time for a LD operation is spent on waiting, and that’s usually hidden via high occupancy), there’re still small benefit of the reduced number of instruction issuance.\n\n\nThe following graph demonstrates the benefit of vectorized loading.\n\n\n\nData movement rate comparison between different size of element read per thread\n\n\nThe actual implementation in CUDA is actually simple, instead of dealing with float type, just deal with float4 , and the compiler will compile it to the correct instruction.\nBeside vectorizing global memory access, we can also notice that there’s a non-coalesced memory access from shared memory when actually performing the per-thread multiplication. Consider the 2 outlined blocks of data that currently resides in shared memory:\n\nthe access to the red block from B is coalesced, because B is row-majored\nthe access to the red block from A is not coalesced, also because A is row-majored\n\nNote that it doesn’t matter how we choose iterate within the thread tile, we can read in a sub-row from A and a sub-column from B and one of them would still be non-coalesced.\n\n\n\nCoalesced access\n\n\nTo solve this, we can simply adopt a transposed shared memory layout.\n\n\n\nTranspose the shared memory layout can result in coalesced read into registers (but non-coalesced write into shared memory, which can be handled via pipelining). Here the region marked by t1 and t2 are indicating what data each thread is responsible of loading from global memory and writing to shared memory. In practice, a thread can handle less than a row in the sub-block of global memory.\n\n\nCombining the vectorization load and coalesced access, we get the following performance\n\n\n\nApproaching 80% CuBLAS performance with vectorization and coalesced access\n\n\n\ncode\n#pragma once\n\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cublas_v2.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define OFFSET(row, col, ld) ((row)*(ld)+(col))\n#ifndef FETCH_FLOAT4\n#define FETCH_FLOAT4(pointer) (reinterpret_cast&lt;float4*&gt;(&(pointer))[0])\n#endif\n\ntemplate&lt;const int BM,\n        const int BN,\n        const int BK,\n        const int TM,\n        const int TN&gt;\n__global__ void sgemm_tt_(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) {\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    const int block_row_thread = BN / TN;\n    const int block_col_thread = BM / TM;\n    const int thread_num = block_row_thread * block_col_thread;\n\n    int tx = (threadIdx.x % block_row_thread) * TN;\n    int ty = (threadIdx.x / block_row_thread) * TM;\n\n    __shared__ float As[BK * BM];\n    __shared__ float Bs[BK * BN];\n\n    const int ldg_a_num = BK * BM / thread_num / 4; \n    const int ldg_b_num = BK * BN / thread_num / 4;\n\n    int a_tile_row = threadIdx.x / (BK / 4);\n    int a_tile_col = threadIdx.x % (BK / 4) * 4;\n    int a_tile_stride = BM / ldg_a_num; \n\n    int b_tile_row = threadIdx.x / (BN / 4);\n    int b_tile_col = threadIdx.x % (BN / 4) * 4;\n    int b_tile_stride = BK / ldg_b_num;\n\n    float accum[TM][TN] = {0.};\n\n    float ldg_a_reg[4 * ldg_a_num] = {0.};\n\n    float a_frag[TM];\n    float b_frag[TN];\n\n    A = &A[by * BM * K];\n    B = &B[bx * BN];\n    C = &C[by * BM * N + bx * BN];\n\n    for (int k = 0; k &lt; K; k += BK) {\n        for (int i = 0; i &lt; BM; i += a_tile_stride) {\n            int ldg_index = i / a_tile_stride * 4; \n            FETCH_FLOAT4(ldg_a_reg[ldg_index]) =\n                    FETCH_FLOAT4(A[OFFSET(a_tile_row + i, a_tile_col, K)]);\n            As[OFFSET(a_tile_col, i + a_tile_row, BM)] = ldg_a_reg[ldg_index];\n            As[OFFSET(a_tile_col + 1, i + a_tile_row, BM)] = ldg_a_reg[ldg_index + 1];\n            As[OFFSET(a_tile_col + 2, i + a_tile_row, BM)] = ldg_a_reg[ldg_index + 2];\n            As[OFFSET(a_tile_col + 3, i + a_tile_row, BM)] = ldg_a_reg[ldg_index + 3];\n        }\n        for (int i = 0; i &lt; BK; i += b_tile_stride) {\n            FETCH_FLOAT4(Bs[OFFSET(b_tile_row + i, b_tile_col, BN)]) =\n                    FETCH_FLOAT4(B[OFFSET(b_tile_row + i, b_tile_col, N)]);\n        }\n        __syncthreads();\n        A += BK;\n        B += BK * N;\n        for (int i = 0; i &lt; BK; i++) {\n            for (int m = 0; m &lt; TM; m += 4) {\n                FETCH_FLOAT4(a_frag[m]) = FETCH_FLOAT4(As[OFFSET(i, ty + m, BM)]);\n            }\n            for (int n = 0; n &lt; TN; n += 4) {\n                FETCH_FLOAT4(b_frag[n]) = FETCH_FLOAT4(Bs[OFFSET(i, tx + n, BN)]);\n            }\n            for (int m = 0; m &lt; TM; m++) {\n                for (int n = 0; n &lt; TN; n++) {\n                    accum[m][n] += a_frag[m] * b_frag[n];\n                }\n            }\n        }\n        __syncthreads();\n    }\n    for (int m = 0; m &lt; TM; m++) {\n        for (int n = 0; n &lt; TN; n += 4) {\n            float4 ctmp = FETCH_FLOAT4(C[OFFSET(ty + m, tx + n, N)]);\n            ctmp.x = alpha * accum[m][n] + beta * ctmp.x;\n            ctmp.y = alpha * accum[m][n + 1] + beta * ctmp.y;\n            ctmp.z = alpha * accum[m][n + 2] + beta * ctmp.z;\n            ctmp.w = alpha * accum[m][n + 3] + beta * ctmp.w;\n            FETCH_FLOAT4(C[OFFSET(ty + m, tx + n, N)]) = ctmp;\n        }\n    }\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#data-prefetchingpipelining",
    "href": "posts/hitchhiker_cuda/index.html#data-prefetchingpipelining",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Data prefetching/pipelining",
    "text": "Data prefetching/pipelining\nIn CUDA, from the point of view of the warp scheduler, most instructions are “asynchronous” in the sense that they all take some cycles to finish.\nFor example\nld.s32 r1, [j];\nld.s32 r2, [k];\nadd.s32 r3, r1, r2;\nst.s32 [i], r3;\nIn this PTX code (PTX is the pseudo-assembly language that C++ CUDA code compiles to), let’s say that the warp scheduler schedules the first line ld.s32 r1, [j];, the data from address [j] will not become ready in the next cycle (and won’t be for around 400 cycles if we are loading from global memory), so the scheduler simply fires away and issues the ld.s32 r2, [k]; without waiting for r1 to be loaded.\nBut when the scheduler sees add.s32 r3, r1, r2; there’s clearly a dependency on r1 and r2 to be available. So this warp will not be eligible for executing the next instruction, and we call it a stalled warp. Instead the scheduler will look at other warps to see if it schedule them. If no warps are eligible for execution, we’ll be wasting execution units, and the number of active warps we talked about in the previous section will reduced, causing a lower occupancy.\nWith this in mind, we can devise a new way to optimize our kernel by prefetching the data and overlapping the fetch with computation. Here’s what I mean\n\n\n\nPrefetching with 2 stages\n\n\nOf course we don’t have to limit ourselves to 2 stages, and in fact, modern CUDA has a nice pipeline interface to work with for multi-stage loading.\n\n\n\nPrefetching with 3 stages\n\n\nNote that we can also do prefetching for the loading from smem to register step.\nTo implement prefetching is actually pretty simple, because the warp scheduler implements it for us. All we need to do is double the size of shared memory and register size and keep an index that points to the current stage of computation.\nWith prefetching, this is the result\n\n\n\nPerformance with prefetching\n\n\n\nCode\n#pragma once\n\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cublas_v2.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#ifndef FETCH_FLOAT4\n#define FETCH_FLOAT4(pointer) (reinterpret_cast&lt;float4 *&gt;(&(pointer))[0])\n#endif\n#ifndef DIV_UP\n#define DIV_UP(m, n) ((m + n - 1) / n)\n#endif\n#define GROUP_SIZE 8\n#define WARP_SIZE 32\n\nnamespace kernel9 {\n\ntemplate&lt;int BM, int BN, int BK&gt;\n__device__ __forceinline__ void gmem_to_smem(float *A, float *B, float smem_a[][BK][BM], float smem_b[][BK][BN], float ldreg_a[][4], float ldreg_b[][4], int a_smem_rounds, int a_stride, int a_smem_x, int a_smem_y, int b_smem_rounds, int b_stride, int b_smem_y, int b_smem_x, int phase)\n{\n#pragma unroll // A: global -&gt; reg buffer\n    for (int i = 0; i &lt; a_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_a[i]) = FETCH_FLOAT4(A[i * a_stride]);\n        smem_a[phase][a_smem_x][a_smem_y + i * a_stride] = ldreg_a[i][0];\n        smem_a[phase][a_smem_x + 1][a_smem_y + i * a_stride] = ldreg_a[i][1];\n        smem_a[phase][a_smem_x + 2][a_smem_y + i * a_stride] = ldreg_a[i][2];\n        smem_a[phase][a_smem_x + 3][a_smem_y + i * a_stride] = ldreg_a[i][3];\n    }\n#pragma unroll // B: global -&gt; reg buffer\n    for (int i = 0; i &lt; b_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_b[i]) = FETCH_FLOAT4(B[i * b_stride]);\n        FETCH_FLOAT4(smem_b[phase][b_smem_y][b_smem_x + i * b_stride]) = FETCH_FLOAT4(ldreg_b[i]);\n    }\n}\n\n__device__ __forceinline__ void gmem_to_reg(float *A, float *B, float ldreg_a[][4], float ldreg_b[][4], int a_smem_rounds, int a_stride, int b_smem_rounds, int b_stride)\n{\n#pragma unroll // A: global -&gt; reg buffer\n    for (int i = 0; i &lt; a_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_a[i]) = FETCH_FLOAT4(A[i * a_stride]);\n    }\n#pragma unroll // B: global -&gt; reg buffer\n    for (int i = 0; i &lt; b_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_b[i]) = FETCH_FLOAT4(B[i * b_stride]);\n    }\n}\n\ntemplate&lt;int BM, int BN, int BK&gt;\n__device__ __forceinline__ void reg_to_smem(float smem_a[][BK][BM], float smem_b[][BK][BN], float ldreg_a[][4], float ldreg_b[][4], int a_smem_rounds, int a_stride, int a_smem_x, int a_smem_y, int b_smem_rounds, int b_stride, int b_smem_y, int b_smem_x, int phase)\n{\n#pragma unroll // A: reg buffer -&gt; smem\n    for (int i = 0; i &lt; a_smem_rounds; ++i)\n    { // note that this is uncoalesce memory write, and only 4 floats * 4 byte/float = 16 bytes per write\n        smem_a[phase][a_smem_x][a_smem_y + i * a_stride] = ldreg_a[i][0];\n        smem_a[phase][a_smem_x + 1][a_smem_y + i * a_stride] = ldreg_a[i][1];\n        smem_a[phase][a_smem_x + 2][a_smem_y + i * a_stride] = ldreg_a[i][2];\n        smem_a[phase][a_smem_x + 3][a_smem_y + i * a_stride] = ldreg_a[i][3];\n    }\n#pragma unroll // B: reg buffer -&gt; smem\n    for (int i = 0; i &lt; b_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(smem_b[phase][b_smem_y][b_smem_x + i * b_stride]) = FETCH_FLOAT4(ldreg_b[i]);\n    }\n}\n\ntemplate&lt;int BM, int BN, int BK, int TM, int TN&gt;\n__device__ __forceinline__ void smem_to_frag(float frag_a[][TM], float frag_b[][TN], float smem_a[][BK][BM], float smem_b[][BK][BN], int frag_phase, int smem_phase, int bk)\n{\n    // int swizzel_id = (threadIdx.x / 4) % 2;\n#pragma unroll \n    for (int i = 0; i &lt; TM; i += 4)\n    {\n        // int swizzel_i = ((i / 4) ^ swizzel_id) * 4;\n        FETCH_FLOAT4(frag_a[frag_phase][i]) = FETCH_FLOAT4(smem_a[smem_phase][bk][threadIdx.y * TM + i]);\n    }\n#pragma unroll\n    for (int i = 0; i &lt; TN; i += 4)\n    {\n        // int swizzel_i = ((i / 4) ^ swizzel_id) * 4;\n        FETCH_FLOAT4(frag_b[frag_phase][i]) = FETCH_FLOAT4(smem_b[smem_phase][bk][threadIdx.x * TN + i]);\n    }\n}\n\n} // namespace kernel 9\n\n// This function assumes B is already transposed\ntemplate &lt;const int BM,\n          const int BN,\n          const int BK,\n          const int TM,\n          const int TN,\n          const int THREAD_NUMS&gt;\n__global__ void __launch_bounds__(THREAD_NUMS, 2) mysgemm_v9(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C)\n{\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    constexpr int threads_per_block = BM / TM * BN / TN;\n    constexpr int a_ele_per_thread_smem = BM * BK / threads_per_block;\n    constexpr int b_ele_per_thread_smem = BK * BN / threads_per_block;\n    constexpr int a_smem_rounds = a_ele_per_thread_smem / 4;\n    constexpr int b_smem_rounds = b_ele_per_thread_smem / 4;\n    constexpr int a_threads_per_row_per_round = BK / 4;\n    int a_stride = threads_per_block / a_threads_per_row_per_round * K;\n    constexpr int b_threads_per_row_per_round = BN / 4;\n    int b_stride = threads_per_block / b_threads_per_row_per_round * N;\n    // int tid = threadIdx.y * blockDim.x + threadIdx.x;\n    // int lane_id = tid % 32;\n    int a_smem_x = ((threadIdx.y * blockDim.x + threadIdx.x) % a_threads_per_row_per_round) * 4;\n    int a_smem_y = (threadIdx.y * blockDim.x + threadIdx.x) / a_threads_per_row_per_round;\n    int b_smem_x = ((threadIdx.y * blockDim.x + threadIdx.x) % b_threads_per_row_per_round) * 4;\n    int b_smem_y = (threadIdx.y * blockDim.x + threadIdx.x) / b_threads_per_row_per_round;\n\n    static_assert((BM * BK) % threads_per_block == 0);\n    static_assert((BK * BN) % threads_per_block == 0);\n    static_assert(a_ele_per_thread_smem % 4 == 0);\n    static_assert(b_ele_per_thread_smem % 4 == 0);\n    static_assert(BK % 4 == 0);\n    static_assert(BN % 4 == 0);\n    static_assert(threads_per_block / a_threads_per_row_per_round &gt;= 1); // at least cover a row per round\n    static_assert(threads_per_block / b_threads_per_row_per_round &gt;= 1); // at least cover a row per round\n    static_assert(TN % 4 == 0); // at least 4 elements per thread and TN is a multiple of 4\n\n    float accum[TM][TN] = {0.};\n\n    __shared__ float smem_a[2][BK][BM]; // a transposed version of A block\n    __shared__ float smem_b[2][BK][BN];\n\n    // register for loading from global mem to smem\n    float ldreg_a[a_smem_rounds][4];\n    float ldreg_b[b_smem_rounds][4];\n\n    // fragment/register for computation\n    float frag_a[2][TM];\n    float frag_b[2][TN];\n\n    // move A to thread start\n    A = &A[by * BM * K + a_smem_y * K + a_smem_x];\n    B = &B[b_smem_y * N + bx * BN + b_smem_x];\n\n    // 1.1 fetch from global to smem, use register as buffer\n    kernel9::gmem_to_smem&lt;BM, BN, BK&gt;(A, B, smem_a, smem_b, ldreg_a, ldreg_b, a_smem_rounds, a_stride, a_smem_x, a_smem_y, b_smem_rounds, b_stride, b_smem_y, b_smem_x, 0);\n    __syncthreads(); // need the sync such that the following fragment can be obtained\n\n    // 1.2 load 0 round of smem-&gt;frag\n    kernel9::smem_to_frag&lt;BM, BN, BK, TM, TN&gt;(frag_a, frag_b, smem_a, smem_b, 0, 0, 0); // load first batch of frag from first block of smem\n    int smem_write_index = 1; // next index of smems to write to\n    int smem_read_index; // read is current write\n\n    // 2. start the blockwise loop\n    for (int k = 0; k &lt; K / BK ; ++k)\n    {\n        // 2.0 fetch from global to smem, use register as buffer\n        if (k + 1 &lt; K / BK) {\n            A += BK; // every iteration, A moves BK to the right\n            B += N * BK; // every iteration, B moves BK * N down\n            kernel9::gmem_to_reg(A, B, ldreg_a, ldreg_b, a_smem_rounds, a_stride, b_smem_rounds, b_stride); // only load to reg, this is non-blocking\n        }\n        // 2.1 use the frag already loaded to compute the outer product, note that we do register prefetching here\n\n        smem_read_index = smem_write_index ^ 1;\n#pragma unroll\n        for (int b_k = 1; b_k &lt; BK; ++b_k) // load one sub row at a time from smem to frag\n        {\n            kernel9::smem_to_frag&lt;BM, BN, BK, TM, TN&gt;(frag_a, frag_b, smem_a, smem_b, b_k % 2, smem_read_index, b_k);\n#pragma unroll\n            for (int i = 0; i &lt; TM; ++i)\n            { // outer product for the previous prefetched frag\n#pragma unroll\n                for (int j = 0; j &lt; TN; ++j)\n                {\n                    accum[i][j] += frag_a[(b_k - 1) % 2][i] * frag_b[(b_k - 1) % 2][j];\n                }\n            }\n        }\n        // 2.2 if there's next block, start loading from reg to smem\n        if (k + 1 &lt; K / BK) {\n            kernel9::reg_to_smem&lt;BM, BN, BK&gt;(smem_a, smem_b, ldreg_a, ldreg_b, a_smem_rounds, a_stride, a_smem_x, a_smem_y, b_smem_rounds, b_stride, b_smem_y, b_smem_x, smem_write_index);\n            __syncthreads();\n            // prefetch a round of fragments from the current write, this will be blocking\n            kernel9::smem_to_frag&lt;BM, BN, BK, TM, TN&gt;(frag_a, frag_b, smem_a, smem_b, 0, smem_write_index, 0);\n            smem_write_index ^= 1; // update next write\n        }\n#pragma unroll\n        for (int i = 0; i &lt; TM; ++i) \n        { // one last round of outer product because we have only done BK - 1 products\n#pragma unroll\n            for (int j = 0; j &lt; TN; ++j)\n            {\n                accum[i][j] += frag_a[(BK - 1) % 2][i] * frag_b[(BK - 1) % 2][j];\n            }\n        }\n    }\n\n    // 3. put the accumulate value down to C\n    // move C to thread tile start\n    C = &C[(by * BM + threadIdx.y * TM) * N + bx * BN + threadIdx.x * TN];\n#pragma unroll\n    for (int i = 0; i &lt; TM; ++i) {\n#pragma unroll\n        for (int j = 0; j &lt; TM; j += 4) {\n            float4 tmp = FETCH_FLOAT4(C[i * N + j]);\n            tmp.x = alpha * accum[i][j] + beta * tmp.x;\n            tmp.y = alpha * accum[i][j + 1] + beta * tmp.y;\n            tmp.z = alpha * accum[i][j + 2] + beta * tmp.z;\n            tmp.w = alpha * accum[i][j + 3] + beta * tmp.w;\n            FETCH_FLOAT4(C[i * N + j]) = tmp;\n        }\n    }\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#swizzle",
    "href": "posts/hitchhiker_cuda/index.html#swizzle",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Swizzle",
    "text": "Swizzle\nI was not particularly impressed with the result achieved by prefetching. If I profile the kernel, I found that the arithmetic intensity is high enough to achieve theoretical full performance, I also saw a good enough occupancy (\\(\\text{active warps} = 3.84\\) ⇒ \\(\\text{occupancy} = \\frac{3.84}{4} = 96\\%\\) ). So the issue should be a “non-algorithmic” one.\n\n\n\n\n\n\nArithmetic intensity is high enough\n\n\n\n\n\n\n\nOccupancy is high\n\n\n\n\n\nLooking closer at the profiler raw output, I noticed that there’s a very hight count of l1text__data_bank_conflicts_pipe_lsu_mem_shared.sum and smsp__sass_l1text_data_bank_conflicts_pipe_lsu_mem_shared_op_st.sum , the former indicates the memory read instructions have a very high “bank conflict”, the latter indicates that memory writes instructions have a very high “bank conflict”.\n\n\n\nProfiling result indicating high bank conflicts\n\n\nThere’re good resources explaining what bank conflicts are, here’s how I understand it. Remember when we discussed vectorization, I said that in CUDA, all global memory access are done in 32-, 64-, or 128-byte transactions? Turns out that a different access constraint is applied on shared memory access.\nIn CUDA, every shared memory access is done via a memory “bank” (think of it as an access point with 32-bit bandwidth per cycle). There’re 32 banks in shared memory for all SMs. The bank that a certain memory access goes through is determined by\n\\[\n\\text{Bank ID} = \\lfloor(\\text{Addr} \\% 128) / 4\\rfloor\n\\]\nwhere we assume the memory is byte-addressable. The 32 banks correspond to 32 threads in a warp, so the best pattern for arrange shared memory access is one where each thread access a different 4-byte (32 bits) in a 128-byte aligned consecutive 128-byte chunk of shared memory.\n\n\n\n\n\nDiagram illustrating bank conflict, source\n\n\nLooking at the way the kernel writes to shared memory, there’s clearly bank conflicts, as shown below\n\n\n\nDiagram illustrating bank conflict during shared memory write. Note that there’s no bank conflict from reading global memory because all read from a warp is coalesced into memory transactions, so the first read from global memory would actually read more data than requested (since each thread requests a non-continuous chunk of data, which will turn into a memory transaction larger than 4 byte), but this is fine because they will reside in L2/L1 cache and will speed up subsequent reads.\n\n\nFrom the perspective of memory banks, it’s called a 4-way bank conflict since each bank has 4 threads accessing it.\nAlso note that the bank conflict happens not only during write to shared memory, but also during read from shared memory.\n\n\n\nDiagram illustrating 4-way bank conflict\n\n\nTo solve this, we can use something called swizzle, which is a way to rearrange bank access such that all banks are utilized in all iterations.\nThe following diagram illustrates the idea.\n\n\n\nDiagram illustrating swizzle operation’s outcome\n\n\nTo actually implement swizzle is as follows:\n\nFor \\(N\\)-way bank conflict, if we conceptualize a particular memory operation to be 2 mappings\n\nOne map from memory address to Bank ID and is given by\n\\[\n\\text{Bank ID}(\\text{Addr}_t) = f(\\text{Addr}_t) = \\lfloor(\\text{Addr}_t \\% 128) / 4\\rfloor\n\\]\nwhere the subscript indicates the memory address is requested from thread \\(t\\)\nOne map from Bank ID and thread ID to bank row (n-th thread to access a particular bank), and is given by\n\\[\n\\text{Bank Row}(t, \\text{Addr}_t) = \\sum_{i=0}^{t-1}1\\cdot (\\text{Bank ID}(\\text{Addr}_i) = \\text{Bank ID}(\\text{Addr}_t))\n\\]\n\nWe can then use the XOR operator to create a new mapping from \\((\\text{Bank ID, Bank Row}) \\rightarrow \\text{Swizzled Bank ID}\\). We can do this because the group of \\(({0, ...,2^{n}}, \\text{XOR})\\) is a closed group and the mapping is bijective.\n\\[\n\\text{Swizzled ID} = \\text{Bank Row} \\oplus \\text{Bank ID}\n\\]\n\n\n\nDiagram illustrating desired swizzle operation, notice how each initial bank id gets mapped to a possibly different swizzled bank id without conflict, and that any bank id is only accessed once in each row. Also note that this swizzle operation can be applied where bank row and bank id are not necessarily the same set of numbers.\n\n\nAfter obtaining the new bank id, convert it back to memory address with\n\\[\n\\text{Swizzled Addr} = f^{-1}(\\text{Swizzled Bank ID})\n\\]\nNote that to calculate \\(f^{-1}\\), we often need more parameters than just the new bank id because \\(f\\) is not bijective.\nAfter obtaining the memory address, the write to and read from shared memory should share the same swizzle pattern to ensure correctness.\n\nAfter applying swizzle to the share memory read operation (due to time constraint, I didn’t apply to memory store), and rerun the profiling\n\n\n\nShare memory conflict significantly reduced, it didn’t reduce to 0 because of reason that will become clear in the subsequent section\n\n\nHowever, the performance is completely disastrous\n\n\n\nPerformance of prefetching + swizzle goes back to round 1\n\n\nThe reason that this happens, despite little bank conflicts, is due to the fact that the swizzling algorithm only works if all the threads being considered come from the same warp, so in an implementation where we don’t distinguish warps (we are just doing 2-D thread tiling within a block), the result from swizzling is neural at best, at worst it causes significant non-coalesced memory access. The result shown above indicates the latter is at play.\nNow we have a proper motivation for warp tiling.\n\ncode for swizzle, relevant parts are highlighted\n#pragma once\n\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cublas_v2.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#ifndef FETCH_FLOAT4\n#define FETCH_FLOAT4(pointer) (reinterpret_cast&lt;float4 *&gt;(&(pointer))[0])\n#endif\n#ifndef DIV_UP\n#define DIV_UP(m, n) ((m + n - 1) / n)\n#endif\n#define GROUP_SIZE 8\n#define WARP_SIZE 32\n\nnamespace kernel9 {\n\ntemplate&lt;int BM, int BN, int BK&gt;\n__device__ __forceinline__ void gmem_to_smem(float *A, float *B, float smem_a[][BK][BM], float smem_b[][BK][BN], float ldreg_a[][4], float ldreg_b[][4], int a_smem_rounds, int a_stride, int a_smem_x, int a_smem_y, int b_smem_rounds, int b_stride, int b_smem_y, int b_smem_x, int phase)\n{\n#pragma unroll // A: global -&gt; reg buffer\n    for (int i = 0; i &lt; a_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_a[i]) = FETCH_FLOAT4(A[i * a_stride]);\n        // int bank_id = a_smem_y;\n        // int bank_row = tid * BK / 128;\n        // int swizzled_a_smem_y = a_smem_y + bank_row * WARP_SIZE * 4 / BK;\n\n        smem_a[phase][a_smem_x][a_smem_y + i * a_stride] = ldreg_a[i][0];\n        smem_a[phase][a_smem_x + 1][a_smem_y + i * a_stride] = ldreg_a[i][1];\n        smem_a[phase][a_smem_x + 2][a_smem_y + i * a_stride] = ldreg_a[i][2];\n        smem_a[phase][a_smem_x + 3][a_smem_y + i * a_stride] = ldreg_a[i][3];\n    }\n#pragma unroll // B: global -&gt; reg buffer\n    for (int i = 0; i &lt; b_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_b[i]) = FETCH_FLOAT4(B[i * b_stride]);\n        FETCH_FLOAT4(smem_b[phase][b_smem_y][b_smem_x + i * b_stride]) = FETCH_FLOAT4(ldreg_b[i]);\n    }\n}\n\n__device__ __forceinline__ void gmem_to_reg(float *A, float *B, float ldreg_a[][4], float ldreg_b[][4], int a_smem_rounds, int a_stride, int b_smem_rounds, int b_stride)\n{\n#pragma unroll // A: global -&gt; reg buffer\n    for (int i = 0; i &lt; a_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_a[i]) = FETCH_FLOAT4(A[i * a_stride]);\n    }\n#pragma unroll // B: global -&gt; reg buffer\n    for (int i = 0; i &lt; b_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(ldreg_b[i]) = FETCH_FLOAT4(B[i * b_stride]);\n    }\n}\n\ntemplate&lt;int BM, int BN, int BK&gt;\n__device__ __forceinline__ void reg_to_smem(float smem_a[][BK][BM], float smem_b[][BK][BN], float ldreg_a[][4], float ldreg_b[][4], int a_smem_rounds, int a_stride, int a_smem_x, int a_smem_y, int b_smem_rounds, int b_stride, int b_smem_y, int b_smem_x, int phase)\n{\n#pragma unroll // A: reg buffer -&gt; smem\n    for (int i = 0; i &lt; a_smem_rounds; ++i)\n    { // note that this is uncoalesce memory write, and only 4 floats * 4 byte/float = 16 bytes per write\n        smem_a[phase][a_smem_x][a_smem_y + i * a_stride] = ldreg_a[i][0];\n        smem_a[phase][a_smem_x + 1][a_smem_y + i * a_stride] = ldreg_a[i][1];\n        smem_a[phase][a_smem_x + 2][a_smem_y + i * a_stride] = ldreg_a[i][2];\n        smem_a[phase][a_smem_x + 3][a_smem_y + i * a_stride] = ldreg_a[i][3];\n    }\n#pragma unroll // B: reg buffer -&gt; smem\n    for (int i = 0; i &lt; b_smem_rounds; ++i)\n    {\n        FETCH_FLOAT4(smem_b[phase][b_smem_y][b_smem_x + i * b_stride]) = FETCH_FLOAT4(ldreg_b[i]);\n    }\n}\n\ntemplate&lt;int BM, int BN, int BK, int TM, int TN&gt;\n__device__ __forceinline__ void smem_to_frag(float frag_a[][TM], float frag_b[][TN], float smem_a[][BK][BM], float smem_b[][BK][BN], int frag_phase, int smem_phase, int bk)\n{\n#pragma unroll \n    for (int i = 0; i &lt; TM; i += 4)\n    {\n        int tmp = (threadIdx.y * TM + i);\n        tmp = ((tmp / WARP_SIZE) ^ ((tmp % WARP_SIZE) / 4)) % 2 * 4;\n        FETCH_FLOAT4(frag_a[frag_phase][tmp]) = FETCH_FLOAT4(smem_a[smem_phase][bk][threadIdx.y * TM + tmp]);\n    }\n#pragma unroll\n    for (int i = 0; i &lt; TN; i += 4)\n    {\n        int tmp = (threadIdx.x * TN + i);\n        tmp = ((tmp / WARP_SIZE) ^ ((tmp % WARP_SIZE) / 4)) % 2 * 4;\n        FETCH_FLOAT4(frag_b[frag_phase][tmp]) = FETCH_FLOAT4(smem_b[smem_phase][bk][threadIdx.x * TN + tmp]);\n    }\n}\n\n} // namespace kernel 9\n\n// This function assumes B is already transposed\ntemplate &lt;const int BM,\n          const int BN,\n          const int BK,\n          const int TM,\n          const int TN,\n          const int THREAD_NUMS&gt;\n__global__ void __launch_bounds__(THREAD_NUMS, 2) mysgemm_v9(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C)\n{\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    constexpr int threads_per_block = BM / TM * BN / TN;\n    constexpr int a_ele_per_thread_smem = BM * BK / threads_per_block;\n    constexpr int b_ele_per_thread_smem = BK * BN / threads_per_block;\n    constexpr int a_smem_rounds = a_ele_per_thread_smem / 4;\n    constexpr int b_smem_rounds = b_ele_per_thread_smem / 4;\n    constexpr int a_threads_per_row_per_round = BK / 4;\n    int a_stride = threads_per_block / a_threads_per_row_per_round * K;\n    constexpr int b_threads_per_row_per_round = BN / 4;\n    int b_stride = threads_per_block / b_threads_per_row_per_round * N;\n    int tid = threadIdx.y * blockDim.x + threadIdx.x;\n    // int lane_id = tid % 32;\n    int a_smem_x = (tid % a_threads_per_row_per_round) * 4;\n    int a_smem_y = tid / a_threads_per_row_per_round;\n    int b_smem_x = (tid % b_threads_per_row_per_round) * 4;\n    int b_smem_y = tid / b_threads_per_row_per_round;\n\n    static_assert((BM * BK) % threads_per_block == 0);\n    static_assert((BK * BN) % threads_per_block == 0);\n    static_assert(a_ele_per_thread_smem % 4 == 0);\n    static_assert(b_ele_per_thread_smem % 4 == 0);\n    static_assert(BK % 4 == 0);\n    static_assert(BN % 4 == 0);\n    static_assert(threads_per_block / a_threads_per_row_per_round &gt;= 1); // at least cover a row per round\n    static_assert(threads_per_block / b_threads_per_row_per_round &gt;= 1); // at least cover a row per round\n    static_assert(TN % 4 == 0); // at least 4 elements per thread and TN is a multiple of 4\n\n    float accum[TM][TN] = {0.};\n\n    __shared__ float smem_a[2][BK][BM]; // a transposed version of A block\n    __shared__ float smem_b[2][BK][BN];\n\n    // register for loading from global mem to smem\n    float ldreg_a[a_smem_rounds][4];\n    float ldreg_b[b_smem_rounds][4];\n\n    // fragment/register for computation\n    float frag_a[2][TM];\n    float frag_b[2][TN];\n\n    // move A to thread start\n    A = &A[by * BM * K + a_smem_y * K + a_smem_x];\n    B = &B[b_smem_y * N + bx * BN + b_smem_x];\n\n    // 1.1 fetch from global to smem, use register as buffer\n    kernel9::gmem_to_smem&lt;BM, BN, BK&gt;(A, B, smem_a, smem_b, ldreg_a, ldreg_b, a_smem_rounds, a_stride, a_smem_x, a_smem_y, b_smem_rounds, b_stride, b_smem_y, b_smem_x, 0);\n    __syncthreads(); // need the sync such that the following fragment can be obtained\n\n    // 1.2 load 0 round of smem-&gt;frag\n    kernel9::smem_to_frag&lt;BM, BN, BK, TM, TN&gt;(frag_a, frag_b, smem_a, smem_b, 0, 0, 0); // load first batch of frag from first block of smem\n    int smem_write_index = 1; // next index of smems to write to\n    int smem_read_index; // read is current write\n\n    // 2. start the blockwise loop\n    for (int k = 0; k &lt; K / BK ; ++k)\n    {\n        // 2.0 fetch from global to smem, use register as buffer\n        if (k + 1 &lt; K / BK) {\n            A += BK; // every iteration, A moves BK to the right\n            B += N * BK; // every iteration, B moves BK * N down\n            kernel9::gmem_to_reg(A, B, ldreg_a, ldreg_b, a_smem_rounds, a_stride, b_smem_rounds, b_stride); // only load to reg, this is non-blocking\n        }\n        // 2.1 use the frag already loaded to compute the outer product, note that we do register prefetching here\n\n        smem_read_index = smem_write_index ^ 1;\n#pragma unroll\n        for (int b_k = 1; b_k &lt; BK; ++b_k) // load one sub row at a time from smem to frag\n        {\n            kernel9::smem_to_frag&lt;BM, BN, BK, TM, TN&gt;(frag_a, frag_b, smem_a, smem_b, b_k % 2, smem_read_index, b_k);\n#pragma unroll\n            for (int i = 0; i &lt; TM; ++i)\n            { // outer product for the previous prefetched frag\n#pragma unroll\n                for (int j = 0; j &lt; TN; ++j)\n                {\n                    accum[i][j] += frag_a[(b_k - 1) % 2][i] * frag_b[(b_k - 1) % 2][j];\n                }\n            }\n        }\n        // 2.2 if there's next block, start loading from reg to smem\n        if (k + 1 &lt; K / BK) {\n            kernel9::reg_to_smem&lt;BM, BN, BK&gt;(smem_a, smem_b, ldreg_a, ldreg_b, a_smem_rounds, a_stride, a_smem_x, a_smem_y, b_smem_rounds, b_stride, b_smem_y, b_smem_x, smem_write_index);\n            __syncthreads();\n            // prefetch a round of fragments from the current write, this will be blocking\n            kernel9::smem_to_frag&lt;BM, BN, BK, TM, TN&gt;(frag_a, frag_b, smem_a, smem_b, 0, smem_write_index, 0);\n            smem_write_index ^= 1; // update next write\n        }\n#pragma unroll\n        for (int i = 0; i &lt; TM; ++i) \n        { // one last round of outer product because we have only done BK - 1 products\n#pragma unroll\n            for (int j = 0; j &lt; TN; ++j)\n            {\n                accum[i][j] += frag_a[(BK - 1) % 2][i] * frag_b[(BK - 1) % 2][j];\n            }\n        }\n    }\n\n    // 3. put the accumulate value down to C\n    // move C to thread tile start\n    C = &C[(by * BM + threadIdx.y * TM) * N + bx * BN + threadIdx.x * TN];\n#pragma unroll\n    for (int i = 0; i &lt; TM; ++i) {\n#pragma unroll\n        for (int j = 0; j &lt; TM; j += 4) {\n            float4 tmp = FETCH_FLOAT4(C[i * N + j]);\n            tmp.x = alpha * accum[i][j] + beta * tmp.x;\n            tmp.y = alpha * accum[i][j + 1] + beta * tmp.y;\n            tmp.z = alpha * accum[i][j + 2] + beta * tmp.z;\n            tmp.w = alpha * accum[i][j + 3] + beta * tmp.w;\n            FETCH_FLOAT4(C[i * N + j]) = tmp;\n        }\n    }\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#warp-tiling",
    "href": "posts/hitchhiker_cuda/index.html#warp-tiling",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Warp Tiling",
    "text": "Warp Tiling\nWarp tiling, as the name suggests, is basically tiling at the warp level. Here’s the visual for the algorithm:\n\n\n\nDiagram for warp tiling\n\n\nThe idea is that instead of treating threads in a block as equivalent, we distinguish them by which warp they come from. Therefore each warp now handles a larger tile (warp tile). To give flexibility to the selection of warp tile sizes, while allowing for the selection of thread tile sizes, we have to allow each thread to process more than one thread tile. Therefore each warp tile is further divided into multiple “subtile”, where each subtile consists of \\(32 \\times \\text{TM}\\times\\text{TN}\\) elements, meaning for each subtile to be calculated, all 32 threads needs to calculate only once. But to fill a whole warp tile, all 32 threads need to move from one subtile to another, causing the zig-zag movement seen above.\nThere’re many benefit to this approach:\n\nHaving the concept of warp tiling allows us to do effective swizzle.\nA strided memory access pattern (from a thread’s perspective) allows for coalesced memory access.\nMost importantly, because we are doing more work per thread (9 thread-tiles per thread, in the example), we are again increasing the arithmetic intensity!\n\nFrom shared memory’s perspective, processing each \\(TM\\times TN\\) elements still requires \\(2K\\) reads from shared memory\nBut from device memory’s perspective, processing each \\(\\#\\text{subtiles}\\times TM \\times TN\\) elements only requires \\(\\text{sm}\\times TM \\times K + \\text{sn}\\times TN \\times K\\) reads from global memory, meaning the arithmetic intensity is now\n\n\\[\n\\frac{\\text{sm}\\cdot\\text{sn}\\cdot\\text{TM}\\cdot\\text{TN}}{K\\cdot(\\text{sm}\\cdot \\text{TM} + \\text{sn}\\cdot\\text{TN})}\n\\]\n\nI’ll leave the calculation for the arithmetic intensity improvement to the reader.\nAfter implementing the warp tiling algorithm, this is the result:\n\n\n\nPerformance of warp tiling\n\n\nNote that there’re a few design decisions I made after experiments:\n\nI intentionally didn’t implement buffering/prefetching/pipelining, because doing so increases the required shared memory per block, the reduction of occupancy caused a bigger issue than the benefit of prefetching.\nDue to time constraint, I didn’t implement swizzling on this kernel, but because of the strided access pattern, I’m only suffering from 2-way bank conflict, which was acceptable for me.\nThe code\n#pragma once\n\n#include &lt;algorithm&gt;\n#include &lt;cassert&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cublas_v2.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#ifndef FETCH_FLOAT4\n#define FETCH_FLOAT4(pointer) (reinterpret_cast&lt;float4 *&gt;(&(pointer))[0])\n#endif\n#ifndef FETCH_FLOAT4_CONST\n#define FETCH_FLOAT4_CONST(pointer) (reinterpret_cast&lt;const float4 *&gt;(&(pointer))[0])\n#endif\n#define GROUP_SIZE 8\n#define WARP_SIZE 32\n\nnamespace kernel10 {\n    template&lt;int BM, int BN, int BK, int lda_m_stride, int ldb_k_stride&gt;\n    __device__ __forceinline__ void gmem_to_smem(const float * A, const float * B, int M, int N, int K, float * smem_a, float * smem_b)\n    {\n        // #pragma unroll // A: global -&gt; reg buffer\n        for (uint i = 0; i + lda_m_stride &lt;= BM; i += lda_m_stride)\n        {\n            const float4 tmp = FETCH_FLOAT4_CONST(A[i * K]);\n            smem_a[i] = tmp.x;\n            smem_a[BM + i] = tmp.y;\n            smem_a[2 * BM + i] = tmp.z;\n            smem_a[3 * BM + i] = tmp.w;\n        }\n        // #pragma unroll // B: global -&gt; reg buffer\n        for (uint i = 0; i + ldb_k_stride &lt;= BK; i += ldb_k_stride)\n        {\n            FETCH_FLOAT4(smem_b[i * BN]) = FETCH_FLOAT4_CONST(B[i * N]);\n        }\n    }\n\n    template &lt;const int BM,\n              const int BN,\n              const int BK,\n              const int WM,\n              const int WN,\n              const int TM,\n              const int TN,\n              const int WM_SUBTILE,\n              const int WN_SUBTILE,\n              const int m_subtiles,\n              const int n_subtiles&gt;\n    __device__ __forceinline__ void warp_matmul(const float *smem_a, const float *smem_b, float *acc, float *frag_a, float *frag_b) {\n        // #pragma unroll\n        for (uint k = 0; k &lt; BK; ++k) { \n            // #pragma unroll\n            for (uint i = 0; i &lt; m_subtiles; ++i) {\n                // #pragma unroll\n                for (uint m = 0; m &lt; TM; m+=1) {\n                    frag_a[i * TM + m] = smem_a[k * BM + i * WM_SUBTILE + m];\n                }\n            }\n            // #pragma unroll\n            for (uint i = 0; i &lt; n_subtiles; ++i) {\n                // #pragma unroll\n                for (uint n = 0; n &lt; TN; n+=1) {\n                    frag_b[i * TN + n] = smem_b[k * BN + i * WN_SUBTILE + n];\n                }\n            }\n            // #pragma unroll\n            for (uint i = 0; i &lt; m_subtiles; ++i) {\n                // #pragma unroll\n                for (uint j = 0; j &lt; n_subtiles; ++j) {\n                    // #pragma unroll\n                    for (uint m = 0; m &lt; TM; ++m) {\n                        // #pragma unroll\n                        for (uint n = 0; n &lt; TN; ++n) {\n                            acc[(i * TM + m) * n_subtiles * TN + j * TN + n] += frag_a[i * TM + m] * frag_b[j * TN + n];\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n} // namespace kernel 10\n\n// WARP tiling without double cache, performing C = alpha * A * B + beta * C\ntemplate &lt;const int BM,\n          const int BN,\n          const int BK,\n          const int WM,\n          const int WN,\n          const int TM,\n          const int TN,\n          const int WM_SUBTILE,\n          const int WN_SUBTILE,\n          const int NUM_THREADS,\n          const int lda_m_stride,\n          const int ldb_k_stride,\n          const int m_subtiles,\n          const int n_subtiles\n          &gt;\n__global__ void __launch_bounds__(NUM_THREADS, 3) mysgemm_v10(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C)\n{\n    // every thread loads 4 floats at a time in stride-fashion\n    const uint warp_m_offset = (threadIdx.x / WARP_SIZE) / (BN / WN) * WM;\n    const uint warp_n_offset = (threadIdx.x / WARP_SIZE) % (BN / WN) * WN;\n    const uint m_idx_a = threadIdx.x * 4 / BK;\n    const uint k_idx_a = threadIdx.x % (BK / 4) * 4;\n    const uint k_idx_b = threadIdx.x * 4 / BN;\n    const uint n_idx_b = threadIdx.x % (BN / 4) * 4;\n    const uint subtile_idx_m = (threadIdx.x % WARP_SIZE) / (WN_SUBTILE / TN) * TM;\n    const uint subtile_idx_n = (threadIdx.x % WARP_SIZE) % (WN_SUBTILE / TN) * TN;\n\n    static_assert(lda_m_stride &gt; 0, \"lda_m_stride must be positive to ensure uniform strides\");\n    static_assert(ldb_k_stride &gt; 0, \"ldb_k_stride must be positive to ensure uniform strides\");\n\n    // declare shared memory\n    __shared__ float smem_a[BK * BM]; // transposed\n    __shared__ float smem_b[BK * BN];\n\n    A += blockIdx.y * BM * K + m_idx_a * K + k_idx_a;\n    B += blockIdx.x * BN + k_idx_b * N + n_idx_b;\n    // move C to the warp start\n    C += (blockIdx.y * BM + warp_m_offset + subtile_idx_m) * N  + blockIdx.x * BN + warp_n_offset + subtile_idx_n;\n\n    // move A and B to thread start for loading, this has nothing to do with warps\n\n    // declare accumulators\n    float acc[m_subtiles * n_subtiles * TM * TN] = {0.};\n\n    // declare fragments\n    float frag_a[m_subtiles * TM] = {0.};\n    float frag_b[n_subtiles * TN] = {0.};\n\n\n    // #pragma unroll\n    for (uint k = 0; k &lt; K; k += BK) {\n        kernel10::gmem_to_smem&lt;BM, BN, BK, lda_m_stride, ldb_k_stride&gt;(A, B, M, N, K, smem_a + k_idx_a * BM + m_idx_a, smem_b + k_idx_b * BN + n_idx_b);\n        __syncthreads();\n        // compute the warp level matmul\n        kernel10::warp_matmul&lt;BM, BN, BK, WM, WN, TM, TN, WM_SUBTILE, WN_SUBTILE, m_subtiles, n_subtiles&gt;(smem_a + warp_m_offset + subtile_idx_m, smem_b + + warp_n_offset + subtile_idx_n, acc, frag_a, frag_b);\n        A += BK;\n        B += BK * N;\n        __syncthreads();\n    }\n\n    // reduce\n\n    for (uint i = 0; i &lt; m_subtiles; ++i) {\n        for (uint j = 0; j &lt; n_subtiles; ++j) {\n            // move C to the subtile start\n            float *C_subtile = C + i * WM_SUBTILE * N + j * WN_SUBTILE;\n            // #pragma unroll\n            for (uint m = 0; m &lt; TM; m += 1) {\n                // #pragma unroll\n                for (uint n = 0; n &lt; TN; n += 4) {\n                    float4 tmp = FETCH_FLOAT4(\n                        C_subtile[m * N + n]);\n                    const int acc_offset = (i * TM + m) * n_subtiles * TN + j * TN + n;\n                    tmp.x = alpha * acc[acc_offset] + beta * tmp.x;\n                    tmp.y = alpha * acc[acc_offset + 1] + beta * tmp.y;\n                    tmp.z = alpha * acc[acc_offset + 2] + beta * tmp.z;\n                    tmp.w = alpha * acc[acc_offset + 3] + beta * tmp.w;\n                    FETCH_FLOAT4(C_subtile[m * N + n]) = tmp;\n                }\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#split-k",
    "href": "posts/hitchhiker_cuda/index.html#split-k",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Split-K",
    "text": "Split-K\nComparing the kernel I wrote with CuBLAS, I noticed how the grid dim of CuBLAS has a third dimension (the (256, 2, 3) number). This suggest that the CuBLAS implementation is also splitting the matmul problem along the K dimension.\n\n\n\nProfiling result from nsight compute, Kernel 2 is CuBLAS, mysgemm_v10 is the warp tiling kernel\n\n\nThe idea with split-K is as follows:\n\nAs opposed to having each block, each warp, each thread iterating throughout the dimension K, we split the K dimension among blocks, so each block only computes a partial result.\nAfter computing a partial result, we kick off another reduction kernel that merges all the partial result together.\nThe hope is that by increasing the number of blocks, we further increase occupancy and this benefit is hopefully larger than the additional overhead that the reduction kernel brings.\n\n\n\n\nDiagram illustrating split-K\n\n\nAfter implementing the split-K algorithm, here’s the performance I got:\n\n\n\nPerformance of Split-K\n\n\nHuh? It got worse than plain warp-tiling. After some tinkering (changing dimensions, etc.), I think the reason for the worse performance is possibly:\n\nI haven’t run a complete sweep of hyper-parameters of this kernel, which is what CuBLAS surely did.\nI’m using the kernel launch as an inter-block synchronization primitive (basically, calling the reduction kernel after the computation kernel in the same stream), but ideally, reduction can also happen in a block-by-block fashion, where as long as a set of blocks that cover the entire K dimension has finished calculation, a reduction kernel can be kicked off along that dimension. I’m not sure how to achieve this though, because afaik CUDA doesn’t have any other inter-block synchronization primitive than kernel launching.\nI’m not using fancy things like stream-k.\nI’m too exhausted to do a proper swizzle on this kernel.\n\nBut anyway, I’m happy that the split-K get implemented and is functionally correct. Here’s the code.\n\nCode for Split-k\n#pragma once\n\n#include &lt;algorithm&gt;\n#include &lt;cassert&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstdlib&gt;\n#include &lt;cublas_v2.h&gt;\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cooperative_groups.h&gt;\n#include &lt;cooperative_groups/memcpy_async.h&gt;\n#include &lt;cooperative_groups/reduce.h&gt;\n#include &lt;cuda/barrier&gt;\n#include &lt;cuda/pipeline&gt;\n\nnamespace cg = cooperative_groups;\n\n#ifndef FETCH_FLOAT4\n#define FETCH_FLOAT4(pointer) (reinterpret_cast&lt;float4 *&gt;(&(pointer))[0])\n#endif\n#ifndef FETCH_FLOAT4_CONST\n#define FETCH_FLOAT4_CONST(pointer) (reinterpret_cast&lt;const float4 *&gt;(&(pointer))[0])\n#endif\n#ifndef DIV_UP\n#define DIV_UP(m, n) (((m) + (n) - 1) / (n))\n#endif\n#define GROUP_SIZE 8\n#define WARP_SIZE 32\n\nnamespace kernel11 {\n    template&lt;int BM, int BN, int BK, int lda_m_stride, int ldb_k_stride&gt;\n    __device__ __forceinline__ void gmem_to_smem(const float * A, const float * B, int M, int N, int K, float * smem_a, float * smem_b)\n    {\n        // #pragma unroll // A: global -&gt; reg buffer\n        for (uint i = 0; i + lda_m_stride &lt;= BM; i += lda_m_stride)\n        {\n            const float4 tmp = FETCH_FLOAT4_CONST(A[i * K]);\n            smem_a[i] = tmp.x;\n            smem_a[BM + i] = tmp.y;\n            smem_a[2 * BM + i] = tmp.z;\n            smem_a[3 * BM + i] = tmp.w;\n        }\n        // #pragma unroll // B: global -&gt; reg buffer\n        for (uint i = 0; i + ldb_k_stride &lt;= BK; i += ldb_k_stride)\n        {\n            FETCH_FLOAT4(smem_b[i * BN]) = FETCH_FLOAT4_CONST(B[i * N]);\n        }\n    }\n\n    template &lt;const int BM,\n              const int BN,\n              const int BK,\n              const int WM,\n              const int WN,\n              const int TM,\n              const int TN,\n              const int WM_SUBTILE,\n              const int WN_SUBTILE,\n              const int m_subtiles,\n              const int n_subtiles&gt;\n    __device__ void warp_matmul(const float *smem_a, const float *smem_b, float *acc, float *frag_a, float *frag_b, int warp_m_offset, int subtile_idx_m, int warp_n_offset, int subtile_idx_n) {\n        smem_a += warp_m_offset + subtile_idx_m;\n        smem_b += warp_n_offset + subtile_idx_n;\n        // #pragma unroll\n        for (uint k = 0; k &lt; BK; ++k) { \n            // #pragma unroll\n            for (uint i = 0; i &lt; m_subtiles; ++i) {\n                // #pragma unroll\n                // for (uint m = 0; m &lt; TM; m+=4) {\n                //     FETCH_FLOAT4(frag_a[i * TM + m]) = FETCH_FLOAT4_CONST(smem_a[k * BM + i * WM_SUBTILE + m]);\n                // }\n                // #pragma unroll\n                for (uint m = 0; m &lt; TM; m+=1) {\n                    frag_a[i * TM + m] = smem_a[k * BM + i * WM_SUBTILE + m];\n                }\n            }\n            // #pragma unroll\n            for (uint i = 0; i &lt; n_subtiles; ++i) {\n                // #pragma unroll\n                // for (uint n = 0; n &lt; TN; n+=4) {\n                //     FETCH_FLOAT4(frag_b[i * TN + n]) = FETCH_FLOAT4_CONST(smem_b[k * BN + i * WN_SUBTILE + n]);\n                // }\n                // #pragma unroll\n                for (uint n = 0; n &lt; TN; n+=1) {\n                    frag_b[i * TN + n] = smem_b[k * BN + i * WN_SUBTILE + n];\n                }\n            }\n            // #pragma unroll\n            for (uint i = 0; i &lt; m_subtiles; ++i) {\n                // #pragma unroll\n                for (uint j = 0; j &lt; n_subtiles; ++j) {\n                    // #pragma unroll\n                    for (uint m = 0; m &lt; TM; ++m) {\n                        // #pragma unroll\n                        for (uint n = 0; n &lt; TN; ++n) {\n                            acc[(i * TM + m) * n_subtiles * TN + j * TN + n] += frag_a[i * TM + m] * frag_b[j * TN + n];\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n} // namespace kernel 11\n\n// WARP tiling without double cache, performing C = alpha * A * B + beta * C\ntemplate &lt;const int BM,\n          const int BN,\n          const int BK,\n          const int SPLIT,\n          const int WM,\n          const int WN,\n          const int TM,\n          const int TN,\n          const int WM_SUBTILE,\n          const int WN_SUBTILE,\n          const int NUM_THREADS,\n          const int lda_m_stride,\n          const int ldb_k_stride,\n          const int m_subtiles,\n          const int n_subtiles\n          &gt;\n__global__ void __launch_bounds__(NUM_THREADS, 2) mysgemm_v11(int M, int N, int K, float alpha, float *A, float *B, float beta, float *tC, float *C)\n{\n    // The strided split K can be visualized as follows:\n    // ┌────────┬────────┬────────┬────────┬────────┬────────┬────────┐\n    // │        │        │        │        │        │        │        │\n    // │ split0 │ split1 │ split0 │ split1 │ split0 │ split1 │ split0 │\n    // │        │        │        │        │        │        │        │\n    // │ block0 │ block1 │ block2 │ block3 │ block4 │ block5 │ block6 │\n    // │        │        │        │        │        │        │        │\n    // └────────┴────────┴────────┴────────┴────────┴────────┴────────┘\n    // The reason for strided splits is that different splits handle BKs in a strided fashion to improve L2 cache hit rate.\n    // Note that there might be remainder blocks left causing imbalanced processing across CTAs, this can be handled via stream-K (https://arxiv.org/pdf/2301.03598), but here we'll just ignore (the imbalance) and process it anyway.\n    // To assist reduction, it's better to store the output from different splits together:\n    //  ┌─────────────────────┐                     \n    //  │    unit0 - split0   │                     \n    //  ├─────────────────────┤                     \n    //  │    unit0 - split1   │                     \n    //  ├─────────────────────┤                     \n    //  │    unit1 - split0   │                     \n    //  ├─────────────────────┤                     \n    //  │    unit1 - split0   │                     \n    //  └─────────────────────┘                     \n\n\n    const uint iters_per_split = DIV_UP(K, BK) / SPLIT; // number of BKs a split handles (at least)\n    const uint last_iter_splits = DIV_UP(K, BK) % SPLIT;\n\n    // every thread loads 4 floats at a time in stride-fashion\n    const uint warp_m_offset = (threadIdx.x / WARP_SIZE) / (BN / WN) * WM;\n    const uint warp_n_offset = (threadIdx.x / WARP_SIZE) % (BN / WN) * WN;\n    const uint m_idx_a = threadIdx.x * 4 / BK;\n    const uint k_idx_a = threadIdx.x % (BK / 4) * 4;\n    const uint k_idx_b = threadIdx.x * 4 / BN;\n    const uint n_idx_b = threadIdx.x % (BN / 4) * 4;\n    const uint subtile_idx_m = (threadIdx.x % WARP_SIZE) / (WN_SUBTILE / TN) * TM;\n    const uint subtile_idx_n = (threadIdx.x % WARP_SIZE) % (WN_SUBTILE / TN) * TN;\n\n\n    static_assert(lda_m_stride &gt; 0, \"lda_m_stride must be positive to ensure uniform strides\");\n    static_assert(ldb_k_stride &gt; 0, \"ldb_k_stride must be positive to ensure uniform strides\");\n\n    // declare shared memory\n    __shared__ float smem_a[BK * BM]; // transposed\n    __shared__ float smem_b[BK * BN];\n\n    // move A and B to thread start for loading, this has nothing to do with warps\n    A += blockIdx.y * BM * K + m_idx_a * K + k_idx_a + blockIdx.z * BK;\n    B += blockIdx.x * BN + k_idx_b * N + n_idx_b + blockIdx.z * BK * N;\n    // move tC to the warp start, tC is the temporary gmem to store splits results\n    tC += ((blockIdx.y * BM + warp_m_offset + subtile_idx_m) * N  + blockIdx.x * BN + warp_n_offset + subtile_idx_n) * SPLIT;\n    // move C to the warp start as well\n    C += (blockIdx.y * BM + warp_m_offset + subtile_idx_m) * N  + blockIdx.x * BN + warp_n_offset + subtile_idx_n;\n\n    // declare accumulators\n    float acc[m_subtiles * n_subtiles * TM * TN] = {0.};\n\n    // declare fragments\n    float frag_a[m_subtiles * TM] = {0.};\n    float frag_b[n_subtiles * TN] = {0.};\n\n\n    // #pragma unroll\n    for (uint it = 0; it &lt; iters_per_split; ++it) {\n        kernel11::gmem_to_smem&lt;BM, BN, BK, lda_m_stride, ldb_k_stride&gt;(A, B, M, N, K, smem_a + k_idx_a * BM + m_idx_a, smem_b + k_idx_b * BN + n_idx_b);\n        __syncthreads();\n        // compute the warp level matmul\n        kernel11::warp_matmul&lt;BM, BN, BK, WM, WN, TM, TN, WM_SUBTILE, WN_SUBTILE, m_subtiles, n_subtiles&gt;(smem_a, smem_b, acc, frag_a, frag_b, warp_m_offset, subtile_idx_m, warp_n_offset, subtile_idx_n);\n        A += BK * SPLIT;\n        B += BK * SPLIT * N;\n        __syncthreads();\n    }\n\n    if (last_iter_splits &gt; 0 && blockIdx.z &lt; last_iter_splits) { // process last iteration\n        kernel11::gmem_to_smem&lt;BM, BN, BK, lda_m_stride, ldb_k_stride&gt;(A, B, M, N, K, smem_a + k_idx_a * BM + m_idx_a, smem_b + k_idx_b * BN + n_idx_b);\n        __syncthreads();\n        // compute the warp level matmul\n        kernel11::warp_matmul&lt;BM, BN, BK, WM, WN, TM, TN, WM_SUBTILE, WN_SUBTILE, m_subtiles, n_subtiles&gt;(smem_a, smem_b, acc, frag_a, frag_b, warp_m_offset, subtile_idx_m, warp_n_offset, subtile_idx_n);\n        __syncthreads();\n    }\n\n    // epilogue: reduce to (temporary) gmem\n    for (uint i = 0; i &lt; m_subtiles; ++i) {\n        for (uint j = 0; j &lt; n_subtiles; ++j) {\n            // move C to the subtile start\n            float *C_subtile = C + (i * WM_SUBTILE * N + j * WN_SUBTILE);\n            float *tC_subtile = tC + (i * WM_SUBTILE * N + j * WN_SUBTILE) * SPLIT;\n            // #pragma unroll\n            for (uint m = 0; m &lt; TM; m += 1) {\n                // #pragma unroll\n                for (uint n = 0; n &lt; TN; n += 4) {\n                    const int acc_offset = (i * TM + m) * n_subtiles * TN + j * TN + n;\n                    if (blockIdx.z == 0) { // only the first block in that split should accumulate from original C matrix\n                        float4 tmp = FETCH_FLOAT4(C_subtile[m * N + n]);\n                        tmp.x = alpha * acc[acc_offset] + beta * tmp.x;\n                        tmp.y = alpha * acc[acc_offset + 1] + beta * tmp.y;\n                        tmp.z = alpha * acc[acc_offset + 2] + beta * tmp.z;\n                        tmp.w = alpha * acc[acc_offset + 3] + beta * tmp.w;\n\n                        tC_subtile[m * N * SPLIT + n * SPLIT] = tmp.x;\n                        tC_subtile[m * N * SPLIT + (n + 1) * SPLIT] = tmp.y;\n                        tC_subtile[m * N * SPLIT + (n + 2) * SPLIT] = tmp.z;\n                        tC_subtile[m * N * SPLIT + (n + 3) * SPLIT] = tmp.w;\n                    } else {\n                        tC_subtile[m * N * SPLIT + n * SPLIT + blockIdx.z] = alpha * acc[acc_offset];\n                        tC_subtile[m * N * SPLIT + (n + 1) * SPLIT + blockIdx.z] = alpha * acc[acc_offset + 1];\n                        tC_subtile[m * N * SPLIT + (n + 2) * SPLIT + blockIdx.z] = alpha * acc[acc_offset + 2];\n                        tC_subtile[m * N * SPLIT + (n + 3) * SPLIT + blockIdx.z] = alpha * acc[acc_offset + 3];\n                    }\n                }\n            }\n        }\n    }\n}\n\ntemplate &lt;int SPLIT,\n          int smem_elements,\n          int stages,\n          int reduction_iters&gt;\n__global__ void reduce_k(const int M, const int N, float* __restrict__ tC, float* __restrict__ C, const int block_iters) {\n    auto grid = cg::this_grid();\n    auto block = cg::this_thread_block(); // data is loaded using block as a group\n    auto tile = cg::tiled_partition&lt;SPLIT&gt;(block); // data is reduced using tile as a group\n\n    extern __shared__ float smem[];\n    uint smem_stage_offsets[stages];\n    float sum[reduction_iters] = {0.0f};\n    for (int s = 0; s &lt; stages; ++s) smem_stage_offsets[s] = s * smem_elements * SPLIT;\n\n    uint gmem_init_offset = blockIdx.x * smem_elements * SPLIT;\n    uint gmem_stride = gridDim.x * smem_elements * SPLIT;\n    uint smem_stride = tile.meta_group_size() * SPLIT;\n\n    __shared__ cuda::pipeline_shared_state&lt;\n        cuda::thread_scope::thread_scope_block,\n        stages\n    &gt; shared_state;\n    auto pipeline = cuda::make_pipeline(block, &shared_state);\n\n    for (uint reduce_iter = 0, fetch_iter = 0; reduce_iter &lt; block_iters; ++reduce_iter) {\n        for (; fetch_iter &lt; block_iters && fetch_iter &lt; (reduce_iter + stages); ++fetch_iter) {\n            pipeline.producer_acquire();\n            uint shared_idx = fetch_iter % stages;\n            cuda::memcpy_async(block,\n                               smem + smem_stage_offsets[shared_idx],\n                               tC + gmem_init_offset + gmem_stride * fetch_iter,\n                               sizeof(float) * smem_elements * SPLIT,\n                               pipeline);\n            pipeline.producer_commit();\n        }\n        pipeline.consumer_wait();\n        uint shared_idx = reduce_iter % stages;\n        uint smem_offset =  tile.meta_group_rank() * SPLIT + tile.thread_rank();\n        for (; smem_offset &lt; smem_elements * SPLIT; smem_offset += smem_stride) {\n            uint element_idx = smem_offset / smem_stride;\n            sum[element_idx] = smem[smem_stage_offsets[shared_idx] + smem_offset];\n            sum[element_idx] = cg::reduce(tile, sum[element_idx], cg::plus&lt;float&gt;());\n            if (tile.thread_rank() == 0) {\n                uint output_offset = blockIdx.x * smem_elements + gridDim.x * smem_elements * reduce_iter + smem_offset / SPLIT;\n                C[output_offset] = sum[element_idx]; // copy to global memory\n            }\n        }\n        // __syncthreads();\n        pipeline.consumer_release();\n    }\n}"
  },
  {
    "objectID": "posts/hitchhiker_cuda/index.html#credits",
    "href": "posts/hitchhiker_cuda/index.html#credits",
    "title": "A hitchhiker’s guide to CUDA programming",
    "section": "Credits",
    "text": "Credits\nA lot of credit goes to (Wang 2024) who has set up the benchmarking code, and (Boehm 2022) who has provided an warp tiling implementation. I added a split-K implementation and wrote my own warp tiling implementation, as well as prefetching implementation. If you just want to see the code, check this repo."
  },
  {
    "objectID": "posts/interactive_fid/index.html",
    "href": "posts/interactive_fid/index.html",
    "title": "Interactive FID",
    "section": "",
    "text": "Imagine that you have 2 curves in a 2-D space, how would you measure the similarity of these 2 curves?\n\n\n\nTwo randomly drawn curves in 2D space\n\n\nThis question turns out to be of great importance, as it helps answer the following question:\n\nIn machine learning, generative models need to be evaluated by comparing the data likelihood of generated output vs. the training dataset\nIn robotics, different movement trajectories need to be compared to evaluate their performance\nIn geographic information systems, trajectories of road, river, movements of animals need to be compared, where a similarity measure needs to be defined\n\nThere’re some general properties we wish the distance measure \\(D\\) to have:\n\nCommutativity: \\(D(A, B) = D(B, A)\\) for curve \\(A\\) and \\(B\\)\nTranslation invariant: \\(D(A+\\lambda, B + \\lambda) = D(A, B)\\), where \\(A+\\lambda\\) is to translating all the points on \\(A\\) by \\(\\lambda\\)\nDefinition of zero: \\(D(A, A) = 0\\)\n\nThere’re also some properties that we want for the specific case of curves:\n\nGlobal instead of local: we want the distance measure to be defined in a global sense, as opposed to relying on specific points on these curve\nContinuous in addition to discrete: we want the distance measure to have a natural extension to continuous curves\nInsensitive to length: we don’t want the distance measure to be a function of the length of either curves\n\nIt would not be trivial to define a such measure. For example, one can naively define the weighted sum/integral of square distances between all point pairs on these curves, i.e.,\n\\[\nD = \\frac{1}{Z}\\int\\int\\lVert A(t) - B(\\tau)\\rVert_2^2d\\tau dt\n\\]\nwhere \\(Z\\) could be a normalizing factor to normalize out the effect of length of these 2 curves (otherwise the longer curves are, the more dissimilar they will be, despite that they can be very similar). However, because a close-form solution for the length of any finite curve might not exist, it doesn’t have a nice close-form expression. Another downside is that this formulation is basically describing “on average, how distant a point in curve A is from a point in curve B”, which might not be ideal.\nConsider the following 2 curves:\n\n\n\nTwo curves with different lengths but almost parallel\n\n\nThese 2 curves are almost parallel, except one has made a rather zigzag “detour”. If we are doing weight average, the distance between these 2 curves will be dominated by the “detour” as the “detour” takes a larger proportion in the upper curve. This might be something we want, but it neglects the fact that these 2 curves are very similar if we don’t look at the detour.\nIs there a way to define a measure such that it doesn’t weight the distance so uniformly? But take into account the overall shape?"
  },
  {
    "objectID": "posts/interactive_fid/index.html#the-question",
    "href": "posts/interactive_fid/index.html#the-question",
    "title": "Interactive FID",
    "section": "",
    "text": "Imagine that you have 2 curves in a 2-D space, how would you measure the similarity of these 2 curves?\n\n\n\nTwo randomly drawn curves in 2D space\n\n\nThis question turns out to be of great importance, as it helps answer the following question:\n\nIn machine learning, generative models need to be evaluated by comparing the data likelihood of generated output vs. the training dataset\nIn robotics, different movement trajectories need to be compared to evaluate their performance\nIn geographic information systems, trajectories of road, river, movements of animals need to be compared, where a similarity measure needs to be defined\n\nThere’re some general properties we wish the distance measure \\(D\\) to have:\n\nCommutativity: \\(D(A, B) = D(B, A)\\) for curve \\(A\\) and \\(B\\)\nTranslation invariant: \\(D(A+\\lambda, B + \\lambda) = D(A, B)\\), where \\(A+\\lambda\\) is to translating all the points on \\(A\\) by \\(\\lambda\\)\nDefinition of zero: \\(D(A, A) = 0\\)\n\nThere’re also some properties that we want for the specific case of curves:\n\nGlobal instead of local: we want the distance measure to be defined in a global sense, as opposed to relying on specific points on these curve\nContinuous in addition to discrete: we want the distance measure to have a natural extension to continuous curves\nInsensitive to length: we don’t want the distance measure to be a function of the length of either curves\n\nIt would not be trivial to define a such measure. For example, one can naively define the weighted sum/integral of square distances between all point pairs on these curves, i.e.,\n\\[\nD = \\frac{1}{Z}\\int\\int\\lVert A(t) - B(\\tau)\\rVert_2^2d\\tau dt\n\\]\nwhere \\(Z\\) could be a normalizing factor to normalize out the effect of length of these 2 curves (otherwise the longer curves are, the more dissimilar they will be, despite that they can be very similar). However, because a close-form solution for the length of any finite curve might not exist, it doesn’t have a nice close-form expression. Another downside is that this formulation is basically describing “on average, how distant a point in curve A is from a point in curve B”, which might not be ideal.\nConsider the following 2 curves:\n\n\n\nTwo curves with different lengths but almost parallel\n\n\nThese 2 curves are almost parallel, except one has made a rather zigzag “detour”. If we are doing weight average, the distance between these 2 curves will be dominated by the “detour” as the “detour” takes a larger proportion in the upper curve. This might be something we want, but it neglects the fact that these 2 curves are very similar if we don’t look at the detour.\nIs there a way to define a measure such that it doesn’t weight the distance so uniformly? But take into account the overall shape?"
  },
  {
    "objectID": "posts/interactive_fid/index.html#fréchet-distance",
    "href": "posts/interactive_fid/index.html#fréchet-distance",
    "title": "Interactive FID",
    "section": "Fréchet Distance",
    "text": "Fréchet Distance\nThe Fréchet Distance is defined as:\n\\[\n\\begin{aligned}\nD(A, B) &:= \\overbrace{\\min_{\\alpha, \\beta}}^{\\text{Taking minimum over function space}}\\max_{t\\in[0, 1]}\\left\\{d(A(\\alpha(t)), B(\\beta(t)))\\right\\}\\\\\n\\text{where }&A, B\\text{ are curves }\\\\& A, B:[0, 1] \\rightarrow \\mathcal{R}^2\\\\\n&\\alpha, \\beta \\text{ are any arbitrary {non-decreasing} scalar function} \\\\&\\alpha, \\beta: [0, 1]\\rightarrow[0,1] \\text{ s.t.}\\quad\\alpha(\\tau_1) \\leq\\alpha(\\tau_2)\\quad \\forall \\tau_1\\leq\\tau_2\\\\\n&t\\in[0, 1]\n\\end{aligned}\n\\]\nIn English, this is to say:\n\n\n\n\n\n\nNote\n\n\n\nLet’s suppose you are walking a dog. You are walking along curve A, the dog is walking along curve B. What’s the shortest leash that allows both you and the dog to finish the walk?\n\n\nI’ve found this explanation quite fascinating, because it gives such a good intuition to an otherwise complicated mathematical definition (especially because it’s doing a min over function space).\nIt’s such a good explanation that I build a demo for this.\n\n\n\nFréchet Distance demo: the distance is indicated by the radius of the circles (all of the same radius); The green lines are the shortest distance from each red points from curve 1 to curve 2, all of the green lines should be shorter than Fréchet distance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ramblings on Deep Learning",
    "section": "",
    "text": "A hitchhiker’s guide to CUDA programming\n\n\n\nCUDA\n\nprogramming\n\ntechnical\n\n\n\nWalkthrough of writing a SGEMM kernel that achieves 95% of cuBLAS performance\n\n\n\n\n\nMar 5, 2025\n\n\nSean Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nSSM lacks sequence mixing\n\n\n\ndeep learning\n\nsequence modeling\n\ntechnical\n\n\n\nEvery architecture contains some implicit trade-offs. My impression is SSMs are a good sequential architecture for modalities where interactions within a sequence matters less than a good compression of past states.\n\n\n\n\n\nMar 2, 2024\n\n\nSean Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive FID\n\n\n\ndeep learning\n\nimage generation\n\n\n\nImagine that you have 2 curves in a 2-D space, how would you measure the similarity of these 2 curves?\n\n\n\n\n\nNov 23, 2023\n\n\nSean Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nEM in a nutshell\n\n\n\ndeep learning\n\nprobability\n\ntechnical\n\n\n\nExplaining the EM algorithm in a nutshell\n\n\n\n\n\nAug 2, 2019\n\n\nSean Zhang\n\n\n\n\n\nNo matching items"
  }
]