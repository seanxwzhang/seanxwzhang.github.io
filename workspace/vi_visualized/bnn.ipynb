{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "# Define the model\n",
    "class BNN(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "    self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.nn.functional.relu(self.fc1(x))\n",
    "    return self.fc2(x)\n",
    "\n",
    "  def guide(self, x):\n",
    "    # Define prior distributions for weights and biases\n",
    "    w1_prior = Normal(torch.zeros(hidden_dim, input_dim), torch.ones(hidden_dim, input_dim))\n",
    "    b1_prior = Normal(torch.zeros(hidden_dim), torch.ones(hidden_dim))\n",
    "    w2_prior = Normal(torch.zeros(output_dim, hidden_dim), torch.ones(output_dim, hidden_dim))\n",
    "    b2_prior = Normal(torch.zeros(output_dim), torch.ones(output_dim))\n",
    "\n",
    "    # Define guide distributions with learnable parameters\n",
    "    w1_loc = pyro.sample(\"w1_loc\", w1_prior)\n",
    "    w1_scale = pyro.sample(\"w1_scale\", Uniform(0.1, 2.0))\n",
    "    b1_loc = pyro.sample(\"b1_loc\", b1_prior)\n",
    "    w2_loc = pyro.sample(\"w2_loc\", w2_prior)\n",
    "    w2_scale = pyro.sample(\"w2_scale\", Uniform(0.1, 2.0))\n",
    "    b2_loc = pyro.sample(\"b2_loc\", b2_prior)\n",
    "\n",
    "    # Apply distributions to model parameters\n",
    "    self.fc1.weight = pyro.sample(\"w1\", Normal(w1_loc, w1_scale))\n",
    "    self.fc1.bias = pyro.sample(\"b1\", Normal(b1_loc, torch.ones(hidden_dim)))\n",
    "    self.fc2.weight = pyro.sample(\"w2\", Normal(w2_loc, w2_scale))\n",
    "    self.fc2.bias = pyro.sample(\"b2\", Normal(b2_loc, torch.ones(output_dim)))\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\"data\", train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=32, shuffle=True)\n",
    "(x_test, y_test) = next(iter(torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\"data\", train=False, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=len(x_test), shuffle=False)))\n",
    "\n",
    "# Define model and guide\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "model = BNN(input_dim, hidden_dim, output_dim)\n",
    "guide = model.guide\n",
    "\n",
    "# Define inference algorithm and loss function\n",
    "nuts_kernel = NUTS(model)\n",
    "loss_fn = pyro.infer.Trace_ELBO()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "  for x_batch, y_batch in x_train:\n",
    "    pyro.util.set_rng_seed(epoch * len(x_train) + i)\n",
    "    with pyro.poutine.trace() as trace:\n",
    "      predictions = model(x_batch)\n",
    "      loss = torch.nn.functional.cross_entropy(predictions, y_batch)\n",
    "    elbo = loss_fn(model, guide, x_batch, y_batch)\n",
    "    nuts_kernel.step(trace, elbo)\n",
    "\n",
    "# Evaluate on test data\n",
    "with torch.no_grad():\n",
    "  predictions = model(x_test)\n",
    "  accuracy = (predictions.argmax(-1) == y_test).sum() / len(y_test)\n",
    "  print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions with uncertainty\n",
    "# ... (requires sampling from posterior predictive distribution)\n",
    "\n",
    "# Note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
